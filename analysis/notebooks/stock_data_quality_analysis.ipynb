{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Data Quality Analysis\n",
    "\n",
    "This notebook analyzes the quality of ingested stock price data and provides insights into data completeness, anomalies, and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Database connection\n",
    "DB_URL = 'postgresql://user:password@localhost/shorted'\n",
    "engine = create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overview of stock price data\n",
    "overview_query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT stock_code) as total_stocks,\n",
    "    COUNT(*) as total_records,\n",
    "    MIN(date) as earliest_date,\n",
    "    MAX(date) as latest_date,\n",
    "    COUNT(DISTINCT date) as total_trading_days\n",
    "FROM stock_prices;\n",
    "\"\"\"\n",
    "\n",
    "overview = pd.read_sql(overview_query, engine)\n",
    "print(\"Stock Price Data Overview:\")\n",
    "overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get per-stock statistics\n",
    "stock_stats_query = \"\"\"\n",
    "SELECT \n",
    "    stock_code,\n",
    "    COUNT(*) as record_count,\n",
    "    MIN(date) as first_date,\n",
    "    MAX(date) as last_date,\n",
    "    COUNT(DISTINCT date) as trading_days,\n",
    "    AVG(volume) as avg_volume,\n",
    "    AVG(close) as avg_price\n",
    "FROM stock_prices\n",
    "GROUP BY stock_code\n",
    "ORDER BY avg_volume DESC;\n",
    "\"\"\"\n",
    "\n",
    "stock_stats = pd.read_sql(stock_stats_query, engine)\n",
    "print(f\"Data available for {len(stock_stats)} stocks\")\n",
    "stock_stats.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Completeness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data by field\n",
    "completeness_query = \"\"\"\n",
    "SELECT \n",
    "    stock_code,\n",
    "    COUNT(*) as total_records,\n",
    "    SUM(CASE WHEN open IS NULL THEN 1 ELSE 0 END) as missing_open,\n",
    "    SUM(CASE WHEN high IS NULL THEN 1 ELSE 0 END) as missing_high,\n",
    "    SUM(CASE WHEN low IS NULL THEN 1 ELSE 0 END) as missing_low,\n",
    "    SUM(CASE WHEN close IS NULL THEN 1 ELSE 0 END) as missing_close,\n",
    "    SUM(CASE WHEN volume IS NULL THEN 1 ELSE 0 END) as missing_volume,\n",
    "    SUM(CASE WHEN adjusted_close IS NULL THEN 1 ELSE 0 END) as missing_adj_close\n",
    "FROM stock_prices\n",
    "GROUP BY stock_code;\n",
    "\"\"\"\n",
    "\n",
    "completeness = pd.read_sql(completeness_query, engine)\n",
    "\n",
    "# Calculate completeness percentages\n",
    "for col in ['open', 'high', 'low', 'close', 'volume', 'adj_close']:\n",
    "    completeness[f'{col}_completeness_%'] = ((completeness['total_records'] - completeness[f'missing_{col}']) / completeness['total_records'] * 100).round(2)\n",
    "\n",
    "# Show stocks with data quality issues\n",
    "print(\"Stocks with missing data:\")\n",
    "completeness[completeness[['missing_open', 'missing_high', 'missing_low', 'missing_close', 'missing_volume']].sum(axis=1) > 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data completeness\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fields = ['open', 'high', 'low', 'close', 'volume', 'adj_close']\n",
    "\n",
    "for idx, field in enumerate(fields):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    completeness_data = completeness[f'{field}_completeness_%'].values\n",
    "    ax.hist(completeness_data, bins=20, edgecolor='black')\n",
    "    ax.set_title(f'{field.capitalize()} Completeness Distribution')\n",
    "    ax.set_xlabel('Completeness %')\n",
    "    ax.set_ylabel('Number of Stocks')\n",
    "    ax.axvline(x=100, color='red', linestyle='--', label='100% Complete')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Issues Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze logged quality issues\n",
    "quality_issues_query = \"\"\"\n",
    "SELECT \n",
    "    stock_code,\n",
    "    COUNT(*) as total_issues,\n",
    "    SUM(CASE WHEN NOT is_complete THEN 1 ELSE 0 END) as incomplete_records,\n",
    "    SUM(CASE WHEN anomaly_detected THEN 1 ELSE 0 END) as anomalies,\n",
    "    array_agg(DISTINCT unnest(missing_fields)) as all_missing_fields\n",
    "FROM stock_data_quality\n",
    "GROUP BY stock_code\n",
    "ORDER BY total_issues DESC;\n",
    "\"\"\"\n",
    "\n",
    "quality_issues = pd.read_sql(quality_issues_query, engine)\n",
    "print(f\"Total stocks with quality issues: {len(quality_issues)}\")\n",
    "quality_issues.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific anomalies\n",
    "anomaly_details_query = \"\"\"\n",
    "SELECT \n",
    "    stock_code,\n",
    "    date,\n",
    "    anomaly_details\n",
    "FROM stock_data_quality\n",
    "WHERE anomaly_detected = TRUE\n",
    "LIMIT 100;\n",
    "\"\"\"\n",
    "\n",
    "anomalies = pd.read_sql(anomaly_details_query, engine)\n",
    "print(\"Sample anomalies detected:\")\n",
    "anomalies.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Price Movement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze extreme price movements\n",
    "price_movements_query = \"\"\"\n",
    "WITH price_changes AS (\n",
    "    SELECT \n",
    "        stock_code,\n",
    "        date,\n",
    "        close,\n",
    "        LAG(close) OVER (PARTITION BY stock_code ORDER BY date) as prev_close,\n",
    "        CASE \n",
    "            WHEN LAG(close) OVER (PARTITION BY stock_code ORDER BY date) > 0 \n",
    "            THEN ((close - LAG(close) OVER (PARTITION BY stock_code ORDER BY date)) / \n",
    "                  LAG(close) OVER (PARTITION BY stock_code ORDER BY date)) * 100\n",
    "            ELSE NULL \n",
    "        END as daily_return\n",
    "    FROM stock_prices\n",
    ")\n",
    "SELECT \n",
    "    stock_code,\n",
    "    COUNT(*) as total_days,\n",
    "    AVG(daily_return) as avg_daily_return,\n",
    "    STDDEV(daily_return) as volatility,\n",
    "    MIN(daily_return) as worst_day,\n",
    "    MAX(daily_return) as best_day,\n",
    "    SUM(CASE WHEN ABS(daily_return) > 10 THEN 1 ELSE 0 END) as extreme_days\n",
    "FROM price_changes\n",
    "WHERE daily_return IS NOT NULL\n",
    "GROUP BY stock_code\n",
    "ORDER BY volatility DESC;\n",
    "\"\"\"\n",
    "\n",
    "price_movements = pd.read_sql(price_movements_query, engine)\n",
    "print(\"Stocks by volatility (highest to lowest):\")\n",
    "price_movements.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize volatility distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(price_movements['volatility'], bins=30, edgecolor='black')\n",
    "plt.xlabel('Daily Volatility (%)')\n",
    "plt.ylabel('Number of Stocks')\n",
    "plt.title('Distribution of Stock Volatility')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(price_movements['avg_daily_return'], price_movements['volatility'], alpha=0.6)\n",
    "plt.xlabel('Average Daily Return (%)')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.title('Risk-Return Profile')\n",
    "plt.axhline(y=0, color='red', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='red', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trading Gaps Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for trading gaps (missing dates)\n",
    "def check_trading_gaps(stock_code, start_date, end_date):\n",
    "    query = f\"\"\"\n",
    "    WITH date_series AS (\n",
    "        SELECT generate_series(\n",
    "            '{start_date}'::date,\n",
    "            '{end_date}'::date,\n",
    "            '1 day'::interval\n",
    "        )::date AS trading_date\n",
    "    ),\n",
    "    actual_dates AS (\n",
    "        SELECT DISTINCT date\n",
    "        FROM stock_prices\n",
    "        WHERE stock_code = '{stock_code}'\n",
    "    )\n",
    "    SELECT \n",
    "        d.trading_date,\n",
    "        CASE WHEN a.date IS NULL THEN 'Missing' ELSE 'Present' END as status,\n",
    "        EXTRACT(DOW FROM d.trading_date) as day_of_week\n",
    "    FROM date_series d\n",
    "    LEFT JOIN actual_dates a ON d.trading_date = a.date\n",
    "    WHERE EXTRACT(DOW FROM d.trading_date) NOT IN (0, 6)  -- Exclude weekends\n",
    "    ORDER BY d.trading_date;\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, engine)\n",
    "\n",
    "# Check gaps for a sample stock\n",
    "sample_stock = 'CBA'\n",
    "gaps_df = check_trading_gaps(sample_stock, '2023-01-01', '2023-12-31')\n",
    "missing_days = gaps_df[gaps_df['status'] == 'Missing']\n",
    "print(f\"Missing trading days for {sample_stock} in 2023: {len(missing_days)}\")\n",
    "if len(missing_days) > 0:\n",
    "    print(\"First 10 missing days:\")\n",
    "    print(missing_days.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Price Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive price chart for a stock\n",
    "def plot_stock_price(stock_code, start_date='2023-01-01'):\n",
    "    query = f\"\"\"\n",
    "    SELECT date, open, high, low, close, volume\n",
    "    FROM stock_prices\n",
    "    WHERE stock_code = '{stock_code}' AND date >= '{start_date}'\n",
    "    ORDER BY date;\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.03,\n",
    "        subplot_titles=(f'{stock_code} Stock Price', 'Volume'),\n",
    "        row_heights=[0.7, 0.3]\n",
    "    )\n",
    "    \n",
    "    # Add candlestick chart\n",
    "    fig.add_trace(\n",
    "        go.Candlestick(\n",
    "            x=df['date'],\n",
    "            open=df['open'],\n",
    "            high=df['high'],\n",
    "            low=df['low'],\n",
    "            close=df['close'],\n",
    "            name='Price'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add volume bars\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df['date'],\n",
    "            y=df['volume'],\n",
    "            name='Volume',\n",
    "            marker_color='lightblue'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'{stock_code} Price and Volume Chart',\n",
    "        height=600,\n",
    "        showlegend=False,\n",
    "        xaxis_rangeslider_visible=False\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Price ($)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Volume\", row=2, col=1)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Plot for sample stock\n",
    "plot_stock_price('CBA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Ingestion Log Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ingestion logs\n",
    "ingestion_log_query = \"\"\"\n",
    "SELECT \n",
    "    batch_id,\n",
    "    data_source,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    stocks_processed,\n",
    "    records_inserted,\n",
    "    records_updated,\n",
    "    errors,\n",
    "    status,\n",
    "    started_at,\n",
    "    completed_at,\n",
    "    EXTRACT(EPOCH FROM (completed_at - started_at)) / 60 as duration_minutes\n",
    "FROM stock_data_ingestion_log\n",
    "ORDER BY started_at DESC\n",
    "LIMIT 20;\n",
    "\"\"\"\n",
    "\n",
    "ingestion_logs = pd.read_sql(ingestion_log_query, engine)\n",
    "print(\"Recent ingestion runs:\")\n",
    "ingestion_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Recommendations and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=== DATA QUALITY SUMMARY ===\")\n",
    "print(f\"\\nTotal stocks analyzed: {len(stock_stats)}\")\n",
    "print(f\"Total price records: {overview['total_records'].values[0]:,}\")\n",
    "print(f\"Date range: {overview['earliest_date'].values[0]} to {overview['latest_date'].values[0]}\")\n",
    "\n",
    "# Completeness summary\n",
    "avg_completeness = {\n",
    "    field: completeness[f'{field}_completeness_%'].mean()\n",
    "    for field in ['open', 'high', 'low', 'close', 'volume', 'adj_close']\n",
    "}\n",
    "\n",
    "print(\"\\nAverage field completeness:\")\n",
    "for field, pct in avg_completeness.items():\n",
    "    print(f\"  {field}: {pct:.1f}%\")\n",
    "\n",
    "# Quality issues summary\n",
    "if len(quality_issues) > 0:\n",
    "    print(f\"\\nStocks with quality issues: {len(quality_issues)}\")\n",
    "    print(f\"Total quality issues logged: {quality_issues['total_issues'].sum()}\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "print(\"1. Address missing data for key fields (open, high, low, volume)\")\n",
    "print(\"2. Investigate stocks with extreme price movements (>10% daily)\")\n",
    "print(\"3. Implement additional data sources for validation\")\n",
    "print(\"4. Set up automated alerts for data quality issues\")\n",
    "print(\"5. Consider implementing data interpolation for minor gaps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}