{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company Metadata Enrichment with GPT-5\n",
    "\n",
    "This notebook enriches ASX company metadata using GPT-5 with Deep Research capabilities.\n",
    "\n",
    "## Features:\n",
    "- Fetch existing metadata from Payload CMS\n",
    "- Generate comprehensive company profiles using GPT-5\n",
    "- Extract company logos from Google Cloud Storage\n",
    "- Fetch annual reports from ASX and company websites\n",
    "- Store enriched data in main Postgres database\n",
    "\n",
    "## Processing:\n",
    "- Supports subset processing for testing\n",
    "- Checkpoint-based resumption\n",
    "- Comprehensive error handling and retry logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dependencies loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Dependencies and Setup\n",
    "import httpx\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import Dict, List, Optional, Any\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úì Dependencies loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  - Process Subset: True\n",
      "  - Subset Size: 3\n",
      "  - Database: aws-0-ap-southeast-2.pooler.supabase.com:5432/postgres\n",
      "  - GCS Base URL: https://storage.googleapis.com/shorted-company-logos/logos\n"
     ]
    }
   ],
   "source": [
    "# OpenAI Configuration\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError('OPENAI_API_KEY environment variable is required. Please set it in your .env file.')\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Database Configuration\n",
    "DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://admin:password@localhost:5432/shorts')\n",
    "CMS_DATABASE_URL = os.getenv('CMS_DATABASE_URL', 'postgresql://admin:password@localhost:5432/cms')\n",
    "\n",
    "# GCS Configuration\n",
    "GCS_BUCKET = os.getenv('GCS_BUCKET', 'shorted-company-logos')\n",
    "GCS_LOGO_BASE_URL = os.getenv('GCS_LOGO_BASE_URL', 'https://storage.googleapis.com/shorted-company-logos/logos')\n",
    "\n",
    "# Processing Configuration\n",
    "PROCESS_SUBSET = os.getenv('PROCESS_SUBSET', 'True').lower() == 'true'\n",
    "SUBSET_SIZE = int(os.getenv('SUBSET_SIZE', '10'))\n",
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE', '50'))\n",
    "CHECKPOINT_INTERVAL = int(os.getenv('CHECKPOINT_INTERVAL', '50'))\n",
    "\n",
    "# API Rate Limiting\n",
    "MAX_RETRIES = int(os.getenv('MAX_RETRIES', '3'))\n",
    "RETRY_DELAY = int(os.getenv('RETRY_DELAY', '5'))\n",
    "\n",
    "# Checkpoint file\n",
    "CHECKPOINT_FILE = 'data/enrichment_checkpoint.json'\n",
    "RESULTS_FILE = 'data/enriched_metadata_results.csv'\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  - Process Subset: {PROCESS_SUBSET}\")\n",
    "print(f\"  - Subset Size: {SUBSET_SIZE}\")\n",
    "print(f\"  - Database: {DATABASE_URL.split('@')[1] if '@' in DATABASE_URL else 'localhost'}\")\n",
    "print(f\"  - GCS Base URL: {GCS_LOGO_BASE_URL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fetched 1954 companies from Payload CMS\n",
      "‚úì Loaded checkpoint: 12 companies processed\n",
      "\n",
      "DataFrame shape: (1954, 11)\n",
      "Columns: ['stock_code', 'company_name', 'industry', 'market_cap', 'listing_date', 'address', 'summary', 'details', 'website', 'company_logo_link', 'logo_gcs_url']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_code</th>\n",
       "      <th>company_name</th>\n",
       "      <th>industry</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>listing_date</th>\n",
       "      <th>address</th>\n",
       "      <th>summary</th>\n",
       "      <th>details</th>\n",
       "      <th>website</th>\n",
       "      <th>company_logo_link</th>\n",
       "      <th>logo_gcs_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MML</td>\n",
       "      <td>MCLAREN MINING LIMITED</td>\n",
       "      <td>Materials</td>\n",
       "      <td>3972048.0</td>\n",
       "      <td>02/05/2022</td>\n",
       "      <td>C/- Argus Corporate Partners, Level 4, 225 St ...</td>\n",
       "      <td>Mining, exploration and development in WA; Tit...</td>\n",
       "      <td>McLaren Mining Limited (ASX:MML formerly Allup...</td>\n",
       "      <td>https://mclarenminerals.com.au</td>\n",
       "      <td>\\thttps://mclarenminerals.com.au/wp-content/th...</td>\n",
       "      <td>https://storage.googleapis.com/shorted-company...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14D</td>\n",
       "      <td>1414 DEGREES LIMITED</td>\n",
       "      <td>Capital Goods</td>\n",
       "      <td>15480954.0</td>\n",
       "      <td>12/09/2018</td>\n",
       "      <td>136 Daws Road, MELROSE PARK, SA, AUSTRALIA, 5039</td>\n",
       "      <td>Commercialising energy storage technology, the...</td>\n",
       "      <td>1414 Degrees Limited (ASX:14D) is an innovativ...</td>\n",
       "      <td>https://www.1414degrees.com.au</td>\n",
       "      <td>https://1414degrees.com.au/wp-content/uploads/...</td>\n",
       "      <td>https://storage.googleapis.com/shorted-company...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29M</td>\n",
       "      <td>29METALS LIMITED</td>\n",
       "      <td>Materials</td>\n",
       "      <td>371900596.0</td>\n",
       "      <td>02/07/2021</td>\n",
       "      <td>Level 2,150 Collins Street, MELBOURNE, VIC, AU...</td>\n",
       "      <td>Mineral exploration, development and productio...</td>\n",
       "      <td>29Metals Limited (ASX:29M) is a copper-focused...</td>\n",
       "      <td>https://www.29metals.com</td>\n",
       "      <td>https://companieslogo.com/img/orig/29M.AX-866b...</td>\n",
       "      <td>https://storage.googleapis.com/shorted-company...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T3D</td>\n",
       "      <td>333D LIMITED</td>\n",
       "      <td>Commercial &amp; Professional Services</td>\n",
       "      <td>836115.0</td>\n",
       "      <td>27/12/2006</td>\n",
       "      <td>Level 23, 525 Collins Street, MELBOURNE, VIC, ...</td>\n",
       "      <td>T3D is a digital asset company merging NFTs an...</td>\n",
       "      <td>333D Limited (ASX:T3D) is Australia's 3D print...</td>\n",
       "      <td>https://www.333d.co</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/153184445...</td>\n",
       "      <td>https://storage.googleapis.com/shorted-company...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TGP</td>\n",
       "      <td>360 CAPITAL GROUP</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>153103307.0</td>\n",
       "      <td>26/07/2005</td>\n",
       "      <td>SUITE 3701 LEVEL 37, 1 MACQUARIE PLACE, SYDNEY...</td>\n",
       "      <td>Real estate investment and funds management.</td>\n",
       "      <td>360 Capital Group (ASX:TGP formerly Trafalgar ...</td>\n",
       "      <td>https://www.360capital.com.au</td>\n",
       "      <td>https://www.360capital.com.au/hubfs/MicrosoftT...</td>\n",
       "      <td>https://storage.googleapis.com/shorted-company...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stock_code             company_name                            industry  \\\n",
       "0        MML   MCLAREN MINING LIMITED                           Materials   \n",
       "1        14D     1414 DEGREES LIMITED                       Capital Goods   \n",
       "2        29M         29METALS LIMITED                           Materials   \n",
       "3        T3D             333D LIMITED  Commercial & Professional Services   \n",
       "4        TGP        360 CAPITAL GROUP                  Financial Services   \n",
       "\n",
       "    market_cap listing_date  \\\n",
       "0    3972048.0   02/05/2022   \n",
       "1   15480954.0   12/09/2018   \n",
       "2  371900596.0   02/07/2021   \n",
       "3     836115.0   27/12/2006   \n",
       "4  153103307.0   26/07/2005   \n",
       "\n",
       "                                             address  \\\n",
       "0  C/- Argus Corporate Partners, Level 4, 225 St ...   \n",
       "1   136 Daws Road, MELROSE PARK, SA, AUSTRALIA, 5039   \n",
       "2  Level 2,150 Collins Street, MELBOURNE, VIC, AU...   \n",
       "3  Level 23, 525 Collins Street, MELBOURNE, VIC, ...   \n",
       "4  SUITE 3701 LEVEL 37, 1 MACQUARIE PLACE, SYDNEY...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Mining, exploration and development in WA; Tit...   \n",
       "1  Commercialising energy storage technology, the...   \n",
       "2  Mineral exploration, development and productio...   \n",
       "3  T3D is a digital asset company merging NFTs an...   \n",
       "4       Real estate investment and funds management.   \n",
       "\n",
       "                                             details  \\\n",
       "0  McLaren Mining Limited (ASX:MML formerly Allup...   \n",
       "1  1414 Degrees Limited (ASX:14D) is an innovativ...   \n",
       "2  29Metals Limited (ASX:29M) is a copper-focused...   \n",
       "3  333D Limited (ASX:T3D) is Australia's 3D print...   \n",
       "4  360 Capital Group (ASX:TGP formerly Trafalgar ...   \n",
       "\n",
       "                          website  \\\n",
       "0  https://mclarenminerals.com.au   \n",
       "1  https://www.1414degrees.com.au   \n",
       "2        https://www.29metals.com   \n",
       "3             https://www.333d.co   \n",
       "4   https://www.360capital.com.au   \n",
       "\n",
       "                                   company_logo_link  \\\n",
       "0  \\thttps://mclarenminerals.com.au/wp-content/th...   \n",
       "1  https://1414degrees.com.au/wp-content/uploads/...   \n",
       "2  https://companieslogo.com/img/orig/29M.AX-866b...   \n",
       "3  https://pbs.twimg.com/profile_images/153184445...   \n",
       "4  https://www.360capital.com.au/hubfs/MicrosoftT...   \n",
       "\n",
       "                                        logo_gcs_url  \n",
       "0  https://storage.googleapis.com/shorted-company...  \n",
       "1  https://storage.googleapis.com/shorted-company...  \n",
       "2  https://storage.googleapis.com/shorted-company...  \n",
       "3  https://storage.googleapis.com/shorted-company...  \n",
       "4  https://storage.googleapis.com/shorted-company...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Data Fetching\n",
    "\n",
    "def fetch_existing_metadata() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch existing company metadata from Payload CMS database, including investor links.\n",
    "    \"\"\"\n",
    "    engine = create_engine(CMS_DATABASE_URL)\n",
    "    \n",
    "    # Fetch base metadata\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        m.id,\n",
    "        m.stock_code,\n",
    "        m.company_name,\n",
    "        m.industry,\n",
    "        m.market_cap,\n",
    "        m.listing_date,\n",
    "        m.address,\n",
    "        m.summary,\n",
    "        m.details,\n",
    "        m.website,\n",
    "        m.company_logo_link\n",
    "    FROM metadata m\n",
    "    WHERE m.stock_code IS NOT NULL\n",
    "    ORDER BY m.company_name\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    # Fetch investor links (2,044 links for 1,931 companies!)\n",
    "    links_query = \"\"\"\n",
    "    SELECT \n",
    "        ml._parent_id,\n",
    "        ml.link,\n",
    "        ml._order\n",
    "    FROM metadata_links ml\n",
    "    ORDER BY ml._parent_id, ml._order\n",
    "    \"\"\"\n",
    "    \n",
    "    df_links = pd.read_sql(links_query, engine)\n",
    "    engine.dispose()\n",
    "    \n",
    "    # Aggregate links per company\n",
    "    if not df_links.empty:\n",
    "        df_links_agg = df_links.groupby('_parent_id')['link'].apply(list).reset_index()\n",
    "        df_links_agg.columns = ['id', 'investor_links']\n",
    "        df = df.merge(df_links_agg, on='id', how='left')\n",
    "    else:\n",
    "        df['investor_links'] = None\n",
    "    \n",
    "    # Fill NaN with empty lists\n",
    "    df['investor_links'] = df['investor_links'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    \n",
    "    # Add logo GCS URLs\n",
    "    df['logo_gcs_url'] = df['stock_code'].apply(\n",
    "        lambda code: f\"{GCS_LOGO_BASE_URL}/{code.upper()}.svg\"\n",
    "    )\n",
    "    \n",
    "    companies_with_links = (df['investor_links'].str.len() > 0).sum()\n",
    "    print(f\"‚úì Fetched {len(df)} companies from Payload CMS\")\n",
    "    print(f\"‚úì {companies_with_links} companies have investor links (avg {df['investor_links'].str.len().mean():.1f} links each)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_checkpoint() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load checkpoint data to resume processing.\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "        print(f\"‚úì Loaded checkpoint: {checkpoint['processed_count']} companies processed\")\n",
    "        return checkpoint\n",
    "    return {'processed_count': 0, 'processed_codes': []}\n",
    "\n",
    "def save_checkpoint(checkpoint: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Save checkpoint data for resumption.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(CHECKPOINT_FILE), exist_ok=True)\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "# Fetch data\n",
    "df_metadata = fetch_existing_metadata()\n",
    "checkpoint = load_checkpoint()\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df_metadata.shape}\")\n",
    "print(f\"Columns: {list(df_metadata.columns)}\")\n",
    "df_metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Schema and prompts defined\n",
      "  Required fields: ['tags', 'enhanced_summary']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: GPT-5 Schema Definition\n",
    "\n",
    "ENRICHMENT_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"tags\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"description\": \"3-7 specific specialty tags describing the company's focus (e.g., 'lithium mining', 'rare earth magnets', 'renewable energy', 'biotech oncology')\"\n",
    "        },\n",
    "        \"enhanced_summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comprehensive company overview (500-1000 words) covering business model, market position, key operations, and strategic focus\"\n",
    "        },\n",
    "        \"company_history\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Historical timeline with key milestones, founding story, major acquisitions, pivots, and evolution (300-500 words)\"\n",
    "        },\n",
    "        \"key_people\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"},\n",
    "                    \"role\": {\"type\": \"string\"},\n",
    "                    \"bio\": {\"type\": \"string\", \"description\": \"2-3 sentence biography with relevant experience\"},\n",
    "                    \"linkedin\": {\"type\": \"string\", \"description\": \"LinkedIn profile URL if available\"}\n",
    "                },\n",
    "                \"required\": [\"name\", \"role\"]\n",
    "            },\n",
    "            \"description\": \"Key executives, board members, and senior leadership\"\n",
    "        },\n",
    "        \"financial_reports\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"type\": {\"type\": \"string\", \"enum\": [\"annual_report\", \"quarterly_report\", \"half_year_report\"]},\n",
    "                    \"date\": {\"type\": \"string\", \"description\": \"Report date in YYYY-MM-DD format\"},\n",
    "                    \"url\": {\"type\": \"string\", \"description\": \"Direct URL to the report PDF\"},\n",
    "                    \"title\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"type\", \"url\"]\n",
    "            },\n",
    "            \"description\": \"Links to recent annual and quarterly reports\"\n",
    "        },\n",
    "        \"competitive_advantages\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Unique strengths, market position, competitive moats, and strategic advantages (200-400 words)\"\n",
    "        },\n",
    "        \"risk_factors\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Key business risks including operational, market, regulatory, and financial risks (200-400 words)\"\n",
    "        },\n",
    "        \"recent_developments\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Recent news, announcements, contracts, or developments from the last 12 months (200-400 words)\"\n",
    "        },\n",
    "        \"social_media_links\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"twitter\": {\"type\": \"string\"},\n",
    "                \"linkedin\": {\"type\": \"string\"},\n",
    "                \"facebook\": {\"type\": \"string\"},\n",
    "                \"youtube\": {\"type\": \"string\"}\n",
    "            },\n",
    "            \"description\": \"Official social media profile URLs\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"tags\", \"enhanced_summary\"]\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a financial research analyst specializing in Australian Stock Exchange (ASX) listed companies.\n",
    "Your task is to provide comprehensive, accurate, and well-researched company profiles.\n",
    "\n",
    "Guidelines:\n",
    "1. Use Deep Research to gather accurate, up-to-date information\n",
    "2. Focus on factual, verifiable information\n",
    "3. Provide specific details rather than generic descriptions\n",
    "4. Include relevant industry context and market positioning\n",
    "5. Cite recent developments and concrete examples\n",
    "6. Maintain professional, objective tone\n",
    "7. For tags, use specific, searchable terms that accurately describe the company's specialty\n",
    "8. Ensure all URLs are valid and publicly accessible\n",
    "\n",
    "Return your response as a valid JSON object matching the provided schema.\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úì Schema and prompts defined\")\n",
    "print(f\"  Required fields: {ENRICHMENT_SCHEMA['required']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì GPT-5 enrichment function defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: GPT-5 Deep Research Function\n",
    "\n",
    "def enrich_company_with_gpt5(company: pd.Series, use_deep_research: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Enrich company metadata using GPT-5 with Deep Research.\n",
    "    \n",
    "    Args:\n",
    "        company: Pandas Series with existing company metadata\n",
    "        use_deep_research: Whether to use Deep Research mode\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with enriched metadata\n",
    "    \"\"\"\n",
    "    stock_code = company['stock_code']\n",
    "    company_name = company['company_name']\n",
    "    \n",
    "    # Prepare context from existing metadata\n",
    "    context = f\"\"\"\n",
    "    Company: {company_name}\n",
    "    ASX Code: {stock_code}\n",
    "    Industry: {company.get('industry', 'N/A')}\n",
    "    Website: {company.get('website', 'N/A')}\n",
    "    Existing Summary: {company.get('summary', 'N/A')}\n",
    "    Address: {company.get('address', 'N/A')}\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Research and provide a comprehensive profile for the following ASX-listed company:\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Please provide detailed, accurate information following the schema. Use Deep Research to find:\n",
    "    - Current company operations and business model\n",
    "    - Recent announcements and developments\n",
    "    - Key leadership team members\n",
    "    - Links to recent annual and quarterly reports\n",
    "    - Company's competitive positioning\n",
    "    - Known risk factors\n",
    "    - Official social media presence\n",
    "    \n",
    "    Focus on factual, verifiable information. For the enhanced_summary, provide a comprehensive\n",
    "    overview that would be suitable for investors and analysts.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Use GPT-5 with structured output\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",  # Will be updated to gpt-5 when available\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0.3,\n",
    "                max_tokens=4000\n",
    "            )\n",
    "            \n",
    "            enriched_data = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            # Add metadata\n",
    "            enriched_data['stock_code'] = stock_code\n",
    "            enriched_data['enrichment_date'] = datetime.now().isoformat()\n",
    "            enriched_data['enrichment_status'] = 'completed'\n",
    "            enriched_data['logo_gcs_url'] = company.get('logo_gcs_url')\n",
    "            \n",
    "            return enriched_data\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö† JSON decode error for {stock_code} (attempt {attempt + 1}/{MAX_RETRIES}): {e}\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Error enriching {stock_code} (attempt {attempt + 1}/{MAX_RETRIES}): {e}\")\n",
    "            if \"rate_limit\" in str(e).lower():\n",
    "                time.sleep(RETRY_DELAY * 2)\n",
    "            elif attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "                continue\n",
    "    \n",
    "    # Return minimal data on failure\n",
    "    return {\n",
    "        'stock_code': stock_code,\n",
    "        'enrichment_status': 'failed',\n",
    "        'enrichment_date': datetime.now().isoformat(),\n",
    "        'enrichment_error': 'Failed after maximum retries'\n",
    "    }\n",
    "\n",
    "print(\"‚úì GPT-5 enrichment function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Annual report fetcher defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Smart Financial Report Crawler\n",
    "\n",
    "def crawl_for_reports(start_url: str, max_depth: int = 2, max_pages: int = 20) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    ENHANCED intelligent crawler to find financial report PDFs.\n",
    "    \n",
    "    Improvements:\n",
    "    - Multiple PDF detection methods (URL patterns, link context, href analysis)\n",
    "    - Better deduplication (URL normalization)\n",
    "    - Smarter link following (priority scoring)\n",
    "    - Handles both direct PDFs and download pages\n",
    "    \n",
    "    Args:\n",
    "        start_url: Starting URL (usually investor relations page)\n",
    "        max_depth: Maximum crawl depth (default: 2 levels)\n",
    "        max_pages: Maximum pages to visit (default: 20)\n",
    "    \n",
    "    Returns:\n",
    "        List of unique report dictionaries\n",
    "    \"\"\"\n",
    "    from urllib.parse import urlparse, urljoin, urlunparse, parse_qs, urlencode\n",
    "    from collections import deque\n",
    "    import re\n",
    "    \n",
    "    reports = []\n",
    "    visited = set()\n",
    "    seen_pdf_urls = set()  # Track unique PDFs\n",
    "    queue = deque([(start_url, 0, 10)])  # (url, depth, priority)\n",
    "    base_domain = urlparse(start_url).netloc\n",
    "    \n",
    "    # HIGH priority keywords (strong signals for financial reports)\n",
    "    high_priority_keywords = [\n",
    "        'annual-report', 'annual_report', 'annualreport',\n",
    "        'financial-report', 'financial_report',\n",
    "        'interim-report', 'quarterly-report',\n",
    "        'investor-reports', 'investor/report'\n",
    "    ]\n",
    "    \n",
    "    # Medium priority keywords\n",
    "    report_keywords = [\n",
    "        'report', 'annual', 'financial', 'investor', \n",
    "        'result', 'presentation', 'disclosure'\n",
    "    ]\n",
    "    \n",
    "    # Avoid these completely\n",
    "    avoid_keywords = [\n",
    "        'login', 'signup', 'register', 'cart', 'checkout',\n",
    "        'subscribe', 'unsubscribe', 'cookie', 'privacy',\n",
    "        'terms', 'condition', 'policy'\n",
    "    ]\n",
    "    \n",
    "    def normalize_url(url: str) -> str:\n",
    "        \"\"\"Normalize URL for deduplication\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        # Remove query parameters that don't affect content\n",
    "        query = parse_qs(parsed.query)\n",
    "        # Keep only meaningful params\n",
    "        cleaned_query = {k: v for k, v in query.items() if k not in ['utm_source', 'utm_medium', 'ei', 'ref']}\n",
    "        new_query = urlencode(cleaned_query, doseq=True)\n",
    "        return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', new_query, ''))\n",
    "    \n",
    "    def extract_year_from_text(text: str) -> str:\n",
    "        \"\"\"Extract year from text, prefer recent years\"\"\"\n",
    "        years = re.findall(r'20\\d{2}', text)\n",
    "        if years:\n",
    "            # Return most recent year found\n",
    "            return max(years)\n",
    "        return ''\n",
    "    \n",
    "    def is_financial_report_link(text: str, href: str) -> bool:\n",
    "        \"\"\"Determine if link text/href indicates a financial report\"\"\"\n",
    "        combined = (text + ' ' + href).lower()\n",
    "        \n",
    "        # Must have report-related keyword\n",
    "        has_report_keyword = any(kw in combined for kw in [\n",
    "            'annual', 'report', 'financial', 'quarter', 'interim', \n",
    "            'full year', 'half year', 'result'\n",
    "        ])\n",
    "        \n",
    "        # Must not have avoid keywords\n",
    "        has_avoid = any(kw in combined for kw in avoid_keywords)\n",
    "        \n",
    "        return has_report_keyword and not has_avoid\n",
    "    \n",
    "    def get_link_priority(url: str, text: str) -> int:\n",
    "        \"\"\"Calculate priority score for following a link\"\"\"\n",
    "        combined = (url + ' ' + text).lower()\n",
    "        score = 5  # Base score\n",
    "        \n",
    "        # High priority paths\n",
    "        if any(kw in combined for kw in high_priority_keywords):\n",
    "            score += 10\n",
    "        \n",
    "        # Medium priority\n",
    "        if any(kw in combined for kw in report_keywords):\n",
    "            score += 5\n",
    "        \n",
    "        # Penalize certain patterns\n",
    "        if any(kw in combined for kw in ['news', 'media', 'blog']):\n",
    "            score -= 5\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    while queue and len(visited) < max_pages:\n",
    "        # Sort queue by priority (highest first)\n",
    "        queue = deque(sorted(queue, key=lambda x: x[2], reverse=True))\n",
    "        current_url, depth, priority = queue.popleft()\n",
    "        \n",
    "        # Normalize and check if visited\n",
    "        norm_url = normalize_url(current_url)\n",
    "        if norm_url in visited or depth > max_depth:\n",
    "            continue\n",
    "        \n",
    "        visited.add(norm_url)\n",
    "        \n",
    "        try:\n",
    "            response = httpx.get(\n",
    "                current_url,\n",
    "                headers={\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "                    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n",
    "                },\n",
    "                timeout=15.0,\n",
    "                follow_redirects=True\n",
    "            )\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract all links\n",
    "            for a_tag in soup.find_all('a', href=True):\n",
    "                href = a_tag['href']\n",
    "                text = a_tag.get_text().strip()\n",
    "                text_lower = text.lower()\n",
    "                \n",
    "                # Resolve relative URLs\n",
    "                full_url = urljoin(current_url, href)\n",
    "                parsed = urlparse(full_url)\n",
    "                \n",
    "                # Only follow links on same domain\n",
    "                if parsed.netloc and parsed.netloc != base_domain:\n",
    "                    continue\n",
    "                \n",
    "                # ENHANCED PDF DETECTION\n",
    "                is_pdf = (\n",
    "                    full_url.lower().endswith('.pdf') or\n",
    "                    '.pdf?' in full_url.lower() or\n",
    "                    'download' in href.lower() and 'pdf' in (text_lower + href.lower()) or\n",
    "                    parsed.path.lower().endswith('.pdf')\n",
    "                )\n",
    "                \n",
    "                if is_pdf and is_financial_report_link(text, href):\n",
    "                    # Normalize PDF URL for deduplication\n",
    "                    norm_pdf_url = normalize_url(full_url)\n",
    "                    \n",
    "                    if norm_pdf_url in seen_pdf_urls:\n",
    "                        continue\n",
    "                    \n",
    "                    seen_pdf_urls.add(norm_pdf_url)\n",
    "                    \n",
    "                    # Extract year\n",
    "                    year = extract_year_from_text(text + ' ' + full_url)\n",
    "                    \n",
    "                    # Determine report type\n",
    "                    report_type = 'annual_report'\n",
    "                    if 'quarterly' in text_lower or 'quarter' in text_lower or 'q1' in text_lower or 'q2' in text_lower or 'q3' in text_lower or 'q4' in text_lower:\n",
    "                        report_type = 'quarterly_report'\n",
    "                    elif 'half year' in text_lower or 'interim' in text_lower or 'half-year' in text_lower:\n",
    "                        report_type = 'half_year_report'\n",
    "                    \n",
    "                    # Clean title\n",
    "                    clean_title = re.sub(r'\\s+', ' ', text).strip()[:100]\n",
    "                    \n",
    "                    reports.append({\n",
    "                        'type': report_type,\n",
    "                        'url': full_url,\n",
    "                        'title': clean_title if clean_title else f\"{year} {report_type}\",\n",
    "                        'date': f\"{year}-06-30\" if year else '',\n",
    "                        'source': 'smart_crawler',\n",
    "                        'depth': depth\n",
    "                    })\n",
    "                \n",
    "                # Should we follow this link?\n",
    "                elif depth < max_depth:\n",
    "                    url_lower = full_url.lower()\n",
    "                    has_avoid = any(kw in url_lower or kw in text_lower for kw in avoid_keywords)\n",
    "                    \n",
    "                    if not has_avoid and full_url not in visited:\n",
    "                        link_priority = get_link_priority(full_url, text)\n",
    "                        \n",
    "                        # Only follow if priority is decent\n",
    "                        if link_priority >= 5:\n",
    "                            queue.append((full_url, depth + 1, link_priority))\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Silently continue on errors\n",
    "            pass\n",
    "    \n",
    "    # Final deduplication and sorting\n",
    "    unique_reports = []\n",
    "    seen_combinations = set()\n",
    "    \n",
    "    for report in reports:\n",
    "        # Create signature: URL + year\n",
    "        year = extract_year_from_text(report['url'] + report['title'])\n",
    "        signature = f\"{normalize_url(report['url'])}_{year}_{report['type']}\"\n",
    "        \n",
    "        if signature not in seen_combinations:\n",
    "            seen_combinations.add(signature)\n",
    "            unique_reports.append(report)\n",
    "    \n",
    "    # Sort by year (most recent first)\n",
    "    unique_reports.sort(key=lambda r: r.get('date', ''), reverse=True)\n",
    "    \n",
    "    return unique_reports\n",
    "\n",
    "def fetch_annual_reports(company: pd.Series) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Fetch annual reports from PayloadCMS investor links, ASX, and company website.\n",
    "    \n",
    "    Priority:\n",
    "    1. PayloadCMS investor links (most reliable - 1,931 companies have these!)\n",
    "    2. ASX announcements API (only returns ~5 recent announcements)\n",
    "    3. Company website fallback\n",
    "    \n",
    "    Args:\n",
    "        company: Pandas Series with company metadata\n",
    "    \n",
    "    Returns:\n",
    "        List of report dictionaries with type, date, url, title\n",
    "    \"\"\"\n",
    "    stock_code = company['stock_code']\n",
    "    reports = []\n",
    "    seen_urls = set()\n",
    "    \n",
    "    def add_report(report_dict):\n",
    "        \"\"\"Helper to avoid duplicates\"\"\"\n",
    "        url = report_dict.get('url', '')\n",
    "        if url and url not in seen_urls:\n",
    "            seen_urls.add(url)\n",
    "            reports.append(report_dict)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    # 1. PRIORITY: Use PayloadCMS investor links with SMART CRAWLER (GOLD MINE!)\n",
    "    investor_links = company.get('investor_links', [])\n",
    "    if isinstance(investor_links, list) and investor_links:\n",
    "        for link in investor_links[:3]:  # Try first 3 links (reduced since we crawl deeper now)\n",
    "            try:\n",
    "                # Skip if obviously not investor-related\n",
    "                if not link or any(x in link.lower() for x in ['facebook', 'twitter', 'linkedin', 'youtube']):\n",
    "                    continue\n",
    "                \n",
    "                # Use smart crawler to traverse the site\n",
    "                crawled_reports = crawl_for_reports(link, max_depth=2, max_pages=15)\n",
    "                \n",
    "                # Add all found reports (crawler already deduplicates within itself)\n",
    "                for report in crawled_reports:\n",
    "                    add_report(report)\n",
    "                \n",
    "                # If we found enough reports, stop\n",
    "                if len(reports) >= 10:\n",
    "                    break\n",
    "                        \n",
    "            except Exception as e:\n",
    "                # Silently continue to next link\n",
    "                pass\n",
    "    \n",
    "    # 2. Try ASX announcements API (backup, only 5 recent announcements)\n",
    "    if len(reports) < 3:  # Only if we don't have enough yet\n",
    "        try:\n",
    "            asx_url = f\"https://cdn-api.markitdigital.com/apiman-gateway/ASX/asx-research/1.0/companies/{stock_code}/announcements\"\n",
    "            response = httpx.get(\n",
    "                asx_url,\n",
    "                headers={\"User-Agent\": \"Mozilla/5.0\"},\n",
    "                timeout=10.0,\n",
    "                follow_redirects=True\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                \n",
    "                if isinstance(data, dict) and 'data' in data:\n",
    "                    api_data = data.get('data', {})\n",
    "                    if isinstance(api_data, dict):\n",
    "                        announcements = api_data.get('items', [])\n",
    "                        \n",
    "                        if isinstance(announcements, list):\n",
    "                            for announcement in announcements:\n",
    "                                if not isinstance(announcement, dict):\n",
    "                                    continue\n",
    "                                    \n",
    "                                title = announcement.get('headline', '').lower()\n",
    "                                if any(kw in title for kw in ['annual report', 'full year', 'quarterly', 'half year']):\n",
    "                                    report_type = 'annual_report' if 'annual' in title or 'full year' in title else 'quarterly_report'\n",
    "                                    if 'half year' in title:\n",
    "                                        report_type = 'half_year_report'\n",
    "                                    \n",
    "                                    add_report({\n",
    "                                        'type': report_type,\n",
    "                                        'date': announcement.get('date', ''),\n",
    "                                        'url': announcement.get('url', ''),\n",
    "                                        'title': announcement.get('headline', ''),\n",
    "                                        'source': 'asx_api'\n",
    "                                    })\n",
    "        except:\n",
    "            pass  # Silently fail, we have other sources\n",
    "    \n",
    "    # 3. Fallback: Try company website\n",
    "    if len(reports) < 2 and not investor_links:  # Only if we really need it\n",
    "        website = company.get('website')\n",
    "        if website and website != 'N/A':\n",
    "            ir_paths = ['/investors', '/investor-relations', '/investor-centre', '/about/investors', '/annual-reports']\n",
    "            \n",
    "            for path in ir_paths:\n",
    "                if len(reports) >= 5:\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    ir_url = urljoin(website, path)\n",
    "                    response = httpx.get(ir_url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10.0, follow_redirects=True)\n",
    "                    \n",
    "                    if response.status_code == 200:\n",
    "                        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                        \n",
    "                        for link in soup.find_all('a', href=True):\n",
    "                            href = link['href']\n",
    "                            text = link.get_text().lower()\n",
    "                            \n",
    "                            if href.endswith('.pdf') and any(kw in text for kw in ['annual', 'report', 'financial']):\n",
    "                                add_report({\n",
    "                                    'type': 'annual_report',\n",
    "                                    'url': urljoin(ir_url, href),\n",
    "                                    'title': link.get_text().strip(),\n",
    "                                    'source': 'website_scrape'\n",
    "                                })\n",
    "                        \n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    return reports[:10]  # Return max 10 reports\n",
    "\n",
    "print(\"‚úì Smart financial report crawler defined\")\n",
    "print(\"  - Crawls up to 2 levels deep\")\n",
    "print(\"  - Intelligently follows report-related links\")\n",
    "print(\"  - Extracts PDFs and metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Resolver functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Resolver Pattern Implementation\n",
    "\n",
    "def resolve_tags(enriched_data: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Validate and clean tags.\n",
    "    \"\"\"\n",
    "    tags = enriched_data.get('tags', [])\n",
    "    \n",
    "    if not isinstance(tags, list):\n",
    "        return []\n",
    "    \n",
    "    # Clean and validate tags\n",
    "    cleaned_tags = []\n",
    "    for tag in tags:\n",
    "        if isinstance(tag, str) and len(tag) > 2 and len(tag) < 50:\n",
    "            cleaned_tags.append(tag.lower().strip())\n",
    "    \n",
    "    return cleaned_tags[:10]  # Max 10 tags\n",
    "\n",
    "def resolve_key_people(enriched_data: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Validate and structure key people data.\n",
    "    \"\"\"\n",
    "    people = enriched_data.get('key_people', [])\n",
    "    \n",
    "    if not isinstance(people, list):\n",
    "        return []\n",
    "    \n",
    "    validated_people = []\n",
    "    for person in people:\n",
    "        if isinstance(person, dict) and 'name' in person and 'role' in person:\n",
    "            validated_people.append({\n",
    "                'name': person['name'],\n",
    "                'role': person['role'],\n",
    "                'bio': person.get('bio', ''),\n",
    "                'linkedin': person.get('linkedin', '')\n",
    "            })\n",
    "    \n",
    "    return validated_people\n",
    "\n",
    "def resolve_financial_reports(enriched_data: Dict[str, Any], company: pd.Series) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Combine GPT-5 results with scraped reports.\n",
    "    \"\"\"\n",
    "    gpt_reports = enriched_data.get('financial_reports', [])\n",
    "    scraped_reports = fetch_annual_reports(company)\n",
    "    \n",
    "    all_reports = []\n",
    "    seen_urls = set()\n",
    "    \n",
    "    # Add both sources, avoiding duplicates\n",
    "    for report in gpt_reports + scraped_reports:\n",
    "        if isinstance(report, dict) and 'url' in report:\n",
    "            url = report['url']\n",
    "            if url and url not in seen_urls:\n",
    "                seen_urls.add(url)\n",
    "                all_reports.append(report)\n",
    "    \n",
    "    return all_reports[:10]  # Max 10 reports\n",
    "\n",
    "def resolve_social_media_links(enriched_data: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Validate social media URLs.\n",
    "    \"\"\"\n",
    "    links = enriched_data.get('social_media_links', {})\n",
    "    \n",
    "    if not isinstance(links, dict):\n",
    "        return {}\n",
    "    \n",
    "    validated_links = {}\n",
    "    url_pattern = re.compile(r'^https?://')\n",
    "    \n",
    "    for platform, url in links.items():\n",
    "        if isinstance(url, str) and url_pattern.match(url):\n",
    "            validated_links[platform] = url\n",
    "    \n",
    "    return validated_links\n",
    "\n",
    "def apply_resolvers(enriched_data: Dict[str, Any], company: pd.Series) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Apply all resolver functions to clean and validate data.\n",
    "    \"\"\"\n",
    "    enriched_data['tags'] = resolve_tags(enriched_data)\n",
    "    enriched_data['key_people'] = resolve_key_people(enriched_data)\n",
    "    enriched_data['financial_reports'] = resolve_financial_reports(enriched_data, company)\n",
    "    enriched_data['social_media_links'] = resolve_social_media_links(enriched_data)\n",
    "    \n",
    "    return enriched_data\n",
    "\n",
    "print(\"‚úì Resolver functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Batch processing function defined\n",
      "  Ready to process companies\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Batch Processing with Subset Support\n",
    "\n",
    "def process_companies(df: pd.DataFrame, checkpoint: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process companies with checkpoint support and progress tracking.\n",
    "    \"\"\"\n",
    "    # Filter companies to process\n",
    "    companies_to_process = df[~df['stock_code'].isin(checkpoint['processed_codes'])]\n",
    "    \n",
    "    if PROCESS_SUBSET:\n",
    "        companies_to_process = companies_to_process.head(SUBSET_SIZE)\n",
    "        print(f\"\\nüìã Processing subset of {len(companies_to_process)} companies\")\n",
    "    else:\n",
    "        print(f\"\\nüìã Processing {len(companies_to_process)} companies (full dataset)\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process with progress bar\n",
    "    for idx, (_, company) in enumerate(tqdm(companies_to_process.iterrows(), total=len(companies_to_process))):\n",
    "        stock_code = company['stock_code']\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nüîç Processing {stock_code} - {company['company_name']}\")\n",
    "            \n",
    "            # Enrich with GPT-5\n",
    "            enriched_data = enrich_company_with_gpt5(company)\n",
    "            \n",
    "            # Apply resolvers\n",
    "            enriched_data = apply_resolvers(enriched_data, company)\n",
    "            \n",
    "            results.append(enriched_data)\n",
    "            \n",
    "            # Update checkpoint\n",
    "            checkpoint['processed_codes'].append(stock_code)\n",
    "            checkpoint['processed_count'] += 1\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if (idx + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "                save_checkpoint(checkpoint)\n",
    "                print(f\"\\nüíæ Checkpoint saved: {checkpoint['processed_count']} companies\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(1)  # Be respectful to APIs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to process {stock_code}: {e}\")\n",
    "            results.append({\n",
    "                'stock_code': stock_code,\n",
    "                'enrichment_status': 'failed',\n",
    "                'enrichment_error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Final checkpoint save\n",
    "    save_checkpoint(checkpoint)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì Batch processing function defined\")\n",
    "print(f\"  Ready to process companies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Validation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Data Validation\n",
    "\n",
    "def validate_enriched_data(results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate and generate statistics for enriched data.\n",
    "    \"\"\"\n",
    "    total = len(results)\n",
    "    completed = sum(1 for r in results if r.get('enrichment_status') == 'completed')\n",
    "    failed = sum(1 for r in results if r.get('enrichment_status') == 'failed')\n",
    "    \n",
    "    # Field coverage\n",
    "    field_coverage = {}\n",
    "    fields_to_check = ['tags', 'enhanced_summary', 'company_history', 'key_people', \n",
    "                       'financial_reports', 'competitive_advantages', 'risk_factors', \n",
    "                       'recent_developments', 'social_media_links']\n",
    "    \n",
    "    for field in fields_to_check:\n",
    "        count = sum(1 for r in results if r.get(field) and r.get('enrichment_status') == 'completed')\n",
    "        field_coverage[field] = {\n",
    "            'count': count,\n",
    "            'percentage': (count / completed * 100) if completed > 0 else 0\n",
    "        }\n",
    "    \n",
    "    # Tag statistics\n",
    "    all_tags = []\n",
    "    for r in results:\n",
    "        if r.get('tags'):\n",
    "            all_tags.extend(r['tags'])\n",
    "    \n",
    "    from collections import Counter\n",
    "    tag_freq = Counter(all_tags)\n",
    "    \n",
    "    validation_report = {\n",
    "        'total_processed': total,\n",
    "        'completed': completed,\n",
    "        'failed': failed,\n",
    "        'success_rate': (completed / total * 100) if total > 0 else 0,\n",
    "        'field_coverage': field_coverage,\n",
    "        'unique_tags': len(tag_freq),\n",
    "        'most_common_tags': tag_freq.most_common(20)\n",
    "    }\n",
    "    \n",
    "    return validation_report\n",
    "\n",
    "def print_validation_report(report: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Print formatted validation report.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä ENRICHMENT VALIDATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Completed: {report['completed']}/{report['total_processed']} ({report['success_rate']:.1f}%)\")\n",
    "    print(f\"‚ùå Failed: {report['failed']}\")\n",
    "    \n",
    "    print(\"\\nüìà Field Coverage:\")\n",
    "    for field, stats in report['field_coverage'].items():\n",
    "        print(f\"  {field:.<30} {stats['count']:>3} ({stats['percentage']:>5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è  Unique Tags: {report['unique_tags']}\")\n",
    "    print(\"\\nMost Common Tags:\")\n",
    "    for tag, count in report['most_common_tags'][:10]:\n",
    "        print(f\"  {tag:.<40} {count:>3}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"‚úì Validation functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Database update function defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Database Update\n",
    "\n",
    "def update_database(results: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Update main Postgres database with enriched data using upsert pattern.\n",
    "    \"\"\"\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    \n",
    "    updated_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    print(f\"\\nüíæ Updating database with {len(results)} records...\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        for result in tqdm(results):\n",
    "            stock_code = result.get('stock_code')\n",
    "            \n",
    "            if not stock_code:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Prepare data for upsert\n",
    "                # Convert list to PostgreSQL array format and JSONB to strings\n",
    "                tags_array = result.get('tags', [])\n",
    "                \n",
    "                update_data = {\n",
    "                    'tags': tags_array if tags_array else None,\n",
    "                    'enhanced_summary': result.get('enhanced_summary'),\n",
    "                    'company_history': result.get('company_history'),\n",
    "                    'key_people': json.dumps(result.get('key_people', [])),\n",
    "                    'financial_reports': json.dumps(result.get('financial_reports', [])),\n",
    "                    'competitive_advantages': result.get('competitive_advantages'),\n",
    "                    'risk_factors': result.get('risk_factors') if isinstance(result.get('risk_factors'), str) else json.dumps(result.get('risk_factors', [])),\n",
    "                    'recent_developments': result.get('recent_developments'),\n",
    "                    'social_media_links': json.dumps(result.get('social_media_links', {})),\n",
    "                    'logo_gcs_url': result.get('logo_gcs_url'),\n",
    "                    'enrichment_status': result.get('enrichment_status', 'completed'),\n",
    "                    'enrichment_date': result.get('enrichment_date', datetime.now().isoformat()),\n",
    "                    'enrichment_error': result.get('enrichment_error'),\n",
    "                    'stock_code': stock_code\n",
    "                }\n",
    "                \n",
    "                # Upsert query (note: table name has hyphen, needs quotes)\n",
    "                # Use :param style for SQLAlchemy text()\n",
    "                # Note: updated_at column doesn't exist in this table, using enrichment_date instead\n",
    "                query = text(\"\"\"\n",
    "                    UPDATE \"company-metadata\"\n",
    "                    SET \n",
    "                        tags = :tags,\n",
    "                        enhanced_summary = :enhanced_summary,\n",
    "                        company_history = :company_history,\n",
    "                        key_people = :key_people,\n",
    "                        financial_reports = :financial_reports,\n",
    "                        competitive_advantages = :competitive_advantages,\n",
    "                        risk_factors = :risk_factors,\n",
    "                        recent_developments = :recent_developments,\n",
    "                        social_media_links = :social_media_links,\n",
    "                        logo_gcs_url = :logo_gcs_url,\n",
    "                        enrichment_status = :enrichment_status,\n",
    "                        enrichment_date = :enrichment_date,\n",
    "                        enrichment_error = :enrichment_error\n",
    "                    WHERE stock_code = :stock_code\n",
    "                \"\"\")\n",
    "                \n",
    "                conn.execute(query, update_data)\n",
    "                conn.commit()\n",
    "                updated_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Failed to update {stock_code}: {e}\")\n",
    "                failed_count += 1\n",
    "    \n",
    "    engine.dispose()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Database update complete:\")\n",
    "    print(f\"  - Updated: {updated_count}\")\n",
    "    print(f\"  - Failed: {failed_count}\")\n",
    "\n",
    "print(\"‚úì Database update function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Export functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Export Results\n",
    "\n",
    "def export_results(results: List[Dict[str, Any]], validation_report: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Export enriched data to CSV and generate summary.\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    os.makedirs(os.path.dirname(RESULTS_FILE), exist_ok=True)\n",
    "    df_results.to_csv(RESULTS_FILE, index=False)\n",
    "    print(f\"\\nüíæ Results exported to: {RESULTS_FILE}\")\n",
    "    \n",
    "    # Save validation report\n",
    "    report_file = RESULTS_FILE.replace('.csv', '_validation_report.json')\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(validation_report, f, indent=2)\n",
    "    print(f\"üìä Validation report saved to: {report_file}\")\n",
    "    \n",
    "    # Generate sample output\n",
    "    if len(df_results) > 0:\n",
    "        print(\"\\nüìÑ Sample Enriched Record:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        sample = df_results[df_results['enrichment_status'] == 'completed'].iloc[0] if len(df_results[df_results['enrichment_status'] == 'completed']) > 0 else df_results.iloc[0]\n",
    "        \n",
    "        print(f\"Stock Code: {sample.get('stock_code')}\")\n",
    "        print(f\"Status: {sample.get('enrichment_status')}\")\n",
    "        print(f\"Tags: {sample.get('tags', [])}\")\n",
    "        print(f\"\\nEnhanced Summary (first 200 chars):\")\n",
    "        summary = sample.get('enhanced_summary', '')\n",
    "        print(summary[:200] + '...' if len(summary) > 200 else summary)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "print(\"‚úì Export functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ STARTING ENRICHMENT PIPELINE\n",
      "============================================================\n",
      "\n",
      "üìã Processing subset of 3 companies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Processing 5GN - 5G NETWORKS LIMITED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:11<00:23, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Processing 88E - 88 ENERGY LIMITED\n",
      "\n",
      "üíæ Checkpoint saved: 14 companies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:25<00:12, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Processing 8CO - 8COMMON LIMITED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:40<00:00, 13.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä ENRICHMENT VALIDATION REPORT\n",
      "============================================================\n",
      "\n",
      "‚úÖ Completed: 3/3 (100.0%)\n",
      "‚ùå Failed: 0\n",
      "\n",
      "üìà Field Coverage:\n",
      "  tags..........................   2 ( 66.7%)\n",
      "  enhanced_summary..............   2 ( 66.7%)\n",
      "  company_history...............   0 (  0.0%)\n",
      "  key_people....................   0 (  0.0%)\n",
      "  financial_reports.............   1 ( 33.3%)\n",
      "  competitive_advantages........   0 (  0.0%)\n",
      "  risk_factors..................   2 ( 66.7%)\n",
      "  recent_developments...........   0 (  0.0%)\n",
      "  social_media_links............   0 (  0.0%)\n",
      "\n",
      "üè∑Ô∏è  Unique Tags: 10\n",
      "\n",
      "Most Common Tags:\n",
      "  cloud services..........................   1\n",
      "  data networks...........................   1\n",
      "  managed it services.....................   1\n",
      "  digital transformation..................   1\n",
      "  australian it services..................   1\n",
      "  expense management software.............   1\n",
      "  saas....................................   1\n",
      "  enterprise software.....................   1\n",
      "  government contracts....................   1\n",
      "  performance management..................   1\n",
      "\n",
      "============================================================\n",
      "\n",
      "üíæ Updating database with 3 records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Database update complete:\n",
      "  - Updated: 3\n",
      "  - Failed: 0\n",
      "\n",
      "üíæ Results exported to: data/enriched_metadata_results.csv\n",
      "üìä Validation report saved to: data/enriched_metadata_results_validation_report.json\n",
      "\n",
      "üìÑ Sample Enriched Record:\n",
      "============================================================\n",
      "Stock Code: 5GN\n",
      "Status: completed\n",
      "Tags: ['cloud services', 'data networks', 'managed it services', 'digital transformation', 'australian it services']\n",
      "\n",
      "Enhanced Summary (first 200 chars):\n",
      "5G Networks Limited (5GN) is an Australian digital services provider specializing in cloud solutions, data networks, and managed IT services. The company is focused on delivering integrated technology...\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "‚úÖ PIPELINE COMPLETE\n",
      "============================================================\n",
      "\n",
      "Processed: 3 companies\n",
      "Success Rate: 100.0%\n",
      "\n",
      "Next steps:\n",
      "  1. Review the validation report\n",
      "  2. Check sample records for quality\n",
      "  3. If satisfied, run with PROCESS_SUBSET=False for full dataset\n",
      "  4. Monitor API costs and adjust as needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Execute Pipeline\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ STARTING ENRICHMENT PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Process companies\n",
    "results = process_companies(df_metadata, checkpoint)\n",
    "\n",
    "# Validate results\n",
    "validation_report = validate_enriched_data(results)\n",
    "print_validation_report(validation_report)\n",
    "\n",
    "# Update database\n",
    "update_database(results)\n",
    "\n",
    "# Export results\n",
    "df_results = export_results(results, validation_report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nProcessed: {len(results)} companies\")\n",
    "print(f\"Success Rate: {validation_report['success_rate']:.1f}%\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(\"  1. Review the validation report\")\n",
    "print(\"  2. Check sample records for quality\")\n",
    "print(\"  3. If satisfied, run with PROCESS_SUBSET=False for full dataset\")\n",
    "print(\"  4. Monitor API costs and adjust as needed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Financial Reports Validation\n",
    "\n",
    "Validate the quality and coverage of fetched financial reports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìà FINANCIAL REPORTS COVERAGE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "‚úÖ Companies with reports: 1/3 (33.3%)\n",
      "‚ùå Companies without reports: 2/3 (66.7%)\n",
      "\n",
      "üìä Report Count Statistics:\n",
      "   Average reports per company: 0.3\n",
      "   Max reports: 1\n",
      "   Min reports: 0\n",
      "\n",
      "üìâ Distribution:\n",
      "   0 reports:   2 companies ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1 reports:   1 companies ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Financial Reports Statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"üìà FINANCIAL REPORTS COVERAGE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Overall statistics\n",
    "total_companies = len(results)\n",
    "companies_with_reports = sum(1 for r in results if r.get('financial_reports'))\n",
    "companies_without_reports = total_companies - companies_with_reports\n",
    "\n",
    "print(f\"\\n‚úÖ Companies with reports: {companies_with_reports}/{total_companies} ({companies_with_reports/total_companies*100:.1f}%)\")\n",
    "print(f\"‚ùå Companies without reports: {companies_without_reports}/{total_companies} ({companies_without_reports/total_companies*100:.1f}%)\")\n",
    "\n",
    "# Report count distribution\n",
    "report_counts = []\n",
    "for r in results:\n",
    "    reports = r.get('financial_reports', [])\n",
    "    if isinstance(reports, str):\n",
    "        try:\n",
    "            reports = json.loads(reports)\n",
    "        except:\n",
    "            reports = []\n",
    "    report_counts.append(len(reports))\n",
    "\n",
    "if report_counts:\n",
    "    print(f\"\\nüìä Report Count Statistics:\")\n",
    "    print(f\"   Average reports per company: {sum(report_counts)/len(report_counts):.1f}\")\n",
    "    print(f\"   Max reports: {max(report_counts)}\")\n",
    "    print(f\"   Min reports: {min(report_counts)}\")\n",
    "    \n",
    "    # Distribution\n",
    "    print(f\"\\nüìâ Distribution:\")\n",
    "    for count in range(0, max(report_counts) + 1):\n",
    "        num_companies = report_counts.count(count)\n",
    "        if num_companies > 0:\n",
    "            bar = \"‚ñà\" * int(num_companies / total_companies * 50)\n",
    "            print(f\"   {count} reports: {num_companies:3d} companies {bar}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç SAMPLE FINANCIAL REPORTS\n",
      "============================================================\n",
      "\n",
      "üìÑ 8CO - 8COMMON LIMITED\n",
      "   Found 1 report(s):\n",
      "\n",
      "   Report #1:\n",
      "      Type:  annual_report\n",
      "      Title: Veritas Securities 8CO Research Report...\n",
      "      Date:  N/A\n",
      "      URL:   https://www.8common.com/wp-content/uploads/2021/07/8CO-Veritas-Research-July-202...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Sample Financial Reports Inspection\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç SAMPLE FINANCIAL REPORTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show detailed reports for first 5 companies with reports\n",
    "sample_count = 0\n",
    "for result in results:\n",
    "    if sample_count >= 5:\n",
    "        break\n",
    "    \n",
    "    reports = result.get('financial_reports', [])\n",
    "    if isinstance(reports, str):\n",
    "        try:\n",
    "            reports = json.loads(reports)\n",
    "        except:\n",
    "            reports = []\n",
    "    \n",
    "    if reports:\n",
    "        stock_code = result.get('stock_code', 'UNKNOWN')\n",
    "        company_name = result.get('company_name', 'Unknown Company')\n",
    "        \n",
    "        print(f\"\\nüìÑ {stock_code} - {company_name}\")\n",
    "        print(f\"   Found {len(reports)} report(s):\")\n",
    "        \n",
    "        for i, report in enumerate(reports, 1):\n",
    "            print(f\"\\n   Report #{i}:\")\n",
    "            print(f\"      Type:  {report.get('type', 'N/A')}\")\n",
    "            print(f\"      Title: {report.get('title', 'N/A')[:80]}...\")\n",
    "            print(f\"      Date:  {report.get('date', 'N/A')}\")\n",
    "            print(f\"      URL:   {report.get('url', 'N/A')[:80]}...\")\n",
    "        \n",
    "        sample_count += 1\n",
    "\n",
    "if sample_count == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No companies with financial reports found in results\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìã REPORT TYPE BREAKDOWN\n",
      "============================================================\n",
      "\n",
      "üìä Report Types Found:\n",
      "   annual_report            :   1 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Report Type Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"üìã REPORT TYPE BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "report_types = {}\n",
    "for result in results:\n",
    "    reports = result.get('financial_reports', [])\n",
    "    if isinstance(reports, str):\n",
    "        try:\n",
    "            reports = json.loads(reports)\n",
    "        except:\n",
    "            reports = []\n",
    "    \n",
    "    for report in reports:\n",
    "        report_type = report.get('type', 'unknown')\n",
    "        report_types[report_type] = report_types.get(report_type, 0) + 1\n",
    "\n",
    "if report_types:\n",
    "    print(f\"\\nüìä Report Types Found:\")\n",
    "    for report_type, count in sorted(report_types.items(), key=lambda x: x[1], reverse=True):\n",
    "        bar = \"‚ñà\" * int(count / sum(report_types.values()) * 50)\n",
    "        print(f\"   {report_type:25s}: {count:3d} {bar}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No reports found to analyze\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚ö†Ô∏è  COMPANIES WITHOUT FINANCIAL REPORTS\n",
      "============================================================\n",
      "\n",
      "Found 2 companies without reports:\n",
      "\n",
      "  1. 5GN    - 5G Networks Limited                               \n",
      "     Website: https://www.5gnetworks.au\n",
      "  2. 88E    - Unknown                                           \n",
      "     Website: N/A\n",
      "\n",
      "üí° Note: GPT-5 Deep Research may have found reports not accessible via ASX API\n",
      "   Check the 'enhanced_summary' and 'company_history' fields for report mentions\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Companies Missing Reports\n",
    "print(\"=\" * 60)\n",
    "print(\"‚ö†Ô∏è  COMPANIES WITHOUT FINANCIAL REPORTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "companies_without_reports = []\n",
    "for result in results:\n",
    "    reports = result.get('financial_reports', [])\n",
    "    if isinstance(reports, str):\n",
    "        try:\n",
    "            reports = json.loads(reports)\n",
    "        except:\n",
    "            reports = []\n",
    "    \n",
    "    if not reports:\n",
    "        companies_without_reports.append({\n",
    "            'stock_code': result.get('stock_code', 'UNKNOWN'),\n",
    "            'company_name': result.get('company_name', 'Unknown'),\n",
    "            'website': result.get('website', 'N/A')\n",
    "        })\n",
    "\n",
    "if companies_without_reports:\n",
    "    print(f\"\\nFound {len(companies_without_reports)} companies without reports:\\n\")\n",
    "    for i, company in enumerate(companies_without_reports, 1):\n",
    "        print(f\"{i:3d}. {company['stock_code']:6s} - {company['company_name'][:50]:50s}\")\n",
    "        print(f\"     Website: {company['website']}\")\n",
    "    \n",
    "    print(f\"\\nüí° Note: GPT-5 Deep Research may have found reports not accessible via ASX API\")\n",
    "    print(f\"   Check the 'enhanced_summary' and 'company_history' fields for report mentions\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All companies have at least one financial report!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report Date Range Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"üìÖ REPORT DATE RANGE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "report_dates = []\n",
    "invalid_dates = 0\n",
    "\n",
    "for result in results:\n",
    "    reports = result.get('financial_reports', [])\n",
    "    if isinstance(reports, str):\n",
    "        try:\n",
    "            reports = json.loads(reports)\n",
    "        except:\n",
    "            reports = []\n",
    "    \n",
    "    for report in reports:\n",
    "        date_str = report.get('date', '')\n",
    "        if date_str:\n",
    "            try:\n",
    "                # Try parsing common date formats\n",
    "                for fmt in ['%Y-%m-%d', '%d/%m/%Y', '%Y/%m/%d', '%d-%m-%Y']:\n",
    "                    try:\n",
    "                        date_obj = datetime.strptime(date_str, fmt)\n",
    "                        report_dates.append(date_obj)\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "                else:\n",
    "                    invalid_dates += 1\n",
    "            except:\n",
    "                invalid_dates += 1\n",
    "\n",
    "if report_dates:\n",
    "    oldest = min(report_dates)\n",
    "    newest = max(report_dates)\n",
    "    \n",
    "    print(f\"\\nüìä Date Range:\")\n",
    "    print(f\"   Oldest report: {oldest.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Newest report: {newest.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Total reports with valid dates: {len(report_dates)}\")\n",
    "    \n",
    "    if invalid_dates > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Reports with invalid dates: {invalid_dates}\")\n",
    "    \n",
    "    # Year distribution\n",
    "    print(f\"\\nüìÖ Reports by Year:\")\n",
    "    years = {}\n",
    "    for date_obj in report_dates:\n",
    "        year = date_obj.year\n",
    "        years[year] = years.get(year, 0) + 1\n",
    "    \n",
    "    for year in sorted(years.keys(), reverse=True):\n",
    "        count = years[year]\n",
    "        bar = \"‚ñà\" * int(count / max(years.values()) * 40)\n",
    "        print(f\"   {year}: {count:3d} {bar}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No valid report dates found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report URL Validation\n",
    "print(\"=\" * 60)\n",
    "print(\"üîó REPORT URL VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "url_sources = {\n",
    "    'ASX API': 0,\n",
    "    'Company Website': 0,\n",
    "    'Other/Invalid': 0\n",
    "}\n",
    "\n",
    "missing_urls = 0\n",
    "duplicate_urls = {}\n",
    "\n",
    "for result in results:\n",
    "    reports = result.get('financial_reports', [])\n",
    "    if isinstance(reports, str):\n",
    "        try:\n",
    "            reports = json.loads(reports)\n",
    "        except:\n",
    "            reports = []\n",
    "    \n",
    "    for report in reports:\n",
    "        url = report.get('url', '')\n",
    "        \n",
    "        if not url or url == 'N/A':\n",
    "            missing_urls += 1\n",
    "        else:\n",
    "            # Track URL sources\n",
    "            if 'asx.com.au' in url.lower():\n",
    "                url_sources['ASX API'] += 1\n",
    "            elif any(domain in url.lower() for domain in ['.com', '.com.au', '.net', '.org']):\n",
    "                url_sources['Company Website'] += 1\n",
    "            else:\n",
    "                url_sources['Other/Invalid'] += 1\n",
    "            \n",
    "            # Check for duplicates\n",
    "            duplicate_urls[url] = duplicate_urls.get(url, 0) + 1\n",
    "\n",
    "print(f\"\\nüìä URL Source Breakdown:\")\n",
    "for source, count in sorted(url_sources.items(), key=lambda x: x[1], reverse=True):\n",
    "    if count > 0:\n",
    "        bar = \"‚ñà\" * int(count / sum(url_sources.values()) * 40) if sum(url_sources.values()) > 0 else \"\"\n",
    "        print(f\"   {source:20s}: {count:3d} {bar}\")\n",
    "\n",
    "if missing_urls > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Reports with missing URLs: {missing_urls}\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = {url: count for url, count in duplicate_urls.items() if count > 1}\n",
    "if duplicates:\n",
    "    print(f\"\\n‚ö†Ô∏è  Duplicate URLs found: {len(duplicates)}\")\n",
    "    for url, count in list(duplicates.items())[:3]:\n",
    "        print(f\"   {url[:70]}... (appears {count}x)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No duplicate URLs found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test Smart Crawler\n",
    "\n",
    "Test the smart crawler on a specific company to see how it finds reports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the smart crawler on 5GN (should find 13 years of reports!)\n",
    "test_company = df_metadata[df_metadata['stock_code'] == '5GN'].iloc[0]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"üß™ TESTING SMART CRAWLER: {test_company['stock_code']} - {test_company['company_name']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "investor_links = test_company.get('investor_links', [])\n",
    "print(f\"\\nüìã Investor links from PayloadCMS: {len(investor_links)}\")\n",
    "for i, link in enumerate(investor_links, 1):\n",
    "    print(f\"   {i}. {link}\")\n",
    "\n",
    "if investor_links:\n",
    "    print(f\"\\nüîç Crawling first link: {investor_links[0]}\")\n",
    "    print(\"   (This may take 10-30 seconds...)\\n\")\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run the smart crawler\n",
    "    reports = crawl_for_reports(investor_links[0], max_depth=2, max_pages=20)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Crawl completed in {elapsed:.1f}s\")\n",
    "    print(f\"üìä Found {len(reports)} financial reports:\\n\")\n",
    "    \n",
    "    if reports:\n",
    "        for i, report in enumerate(reports, 1):\n",
    "            print(f\"   {i:2d}. [{report['type']:20s}] {report['title'][:60]}\")\n",
    "            print(f\"       URL: {report['url'][:80]}\")\n",
    "            print(f\"       Date: {report.get('date', 'N/A'):12s} | Depth: {report.get('depth', 0)} | Source: {report.get('source', 'N/A')}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No reports found - the crawler may need adjustment\")\n",
    "    \n",
    "    print(f\"\\nüí° The full enrichment will use this data plus ASX API and GPT-5 research\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No investor links found for this company\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Results Summary:\n",
      "Total records: 3\n",
      "\n",
      "Status distribution:\n",
      "enrichment_status\n",
      "completed    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üè∑Ô∏è  Companies with most tags:\n",
      "  stock_code  tag_count                                               tags\n",
      "0        5GN          5  [cloud services, data networks, managed it ser...\n",
      "2        8CO          5  [expense management software, saas, enterprise...\n",
      "1        88E          0                                                 []\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Optional - View Results\n",
    "\n",
    "# Display results summary\n",
    "if 'df_results' in locals():\n",
    "    print(\"\\nüìä Results Summary:\")\n",
    "    print(f\"Total records: {len(df_results)}\")\n",
    "    print(f\"\\nStatus distribution:\")\n",
    "    print(df_results['enrichment_status'].value_counts())\n",
    "    \n",
    "    # Show companies with most tags\n",
    "    if 'tags' in df_results.columns:\n",
    "        df_results['tag_count'] = df_results['tags'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        print(\"\\nüè∑Ô∏è  Companies with most tags:\")\n",
    "        print(df_results[['stock_code', 'tag_count', 'tags']].sort_values('tag_count', ascending=False).head(10))\n",
    "    \n",
    "    # Display full DataFrame\n",
    "    df_results.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
