{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company Metadata Enrichment with GPT-5\n",
    "\n",
    "This notebook enriches ASX company metadata using GPT-5 with Deep Research capabilities.\n",
    "\n",
    "## Features:\n",
    "- Fetch existing metadata from Payload CMS\n",
    "- Generate comprehensive company profiles using GPT-5\n",
    "- Extract company logos from Google Cloud Storage\n",
    "- Fetch annual reports from ASX and company websites\n",
    "- Store enriched data in main Postgres database\n",
    "\n",
    "## Processing:\n",
    "- Supports subset processing for testing\n",
    "- Checkpoint-based resumption\n",
    "- Comprehensive error handling and retry logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Dependencies and Setup\n",
    "import httpx\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import Dict, List, Optional, Any\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úì Dependencies loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Cell 2: Configuration\n",
    "\n",
    "# OpenAI Configuration\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError('OPENAI_API_KEY environment variable is required. Please set it in your .env file.')\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Database Configuration\n",
    "DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://admin:password@localhost:5432/shorts')\n",
    "CMS_DATABASE_URL = os.getenv('CMS_DATABASE_URL', 'postgresql://admin:password@localhost:5432/cms')\n",
    "\n",
    "# GCS Configuration\n",
    "GCS_BUCKET = os.getenv('GCS_BUCKET', 'shorted-company-logos')\n",
    "GCS_LOGO_BASE_URL = os.getenv('GCS_LOGO_BASE_URL', 'https://storage.googleapis.com/shorted-company-logos/logos')\n",
    "\n",
    "# Processing Configuration\n",
    "PROCESS_SUBSET = os.getenv('PROCESS_SUBSET', 'True').lower() == 'true'\n",
    "SUBSET_SIZE = int(os.getenv('SUBSET_SIZE', '10'))\n",
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE', '50'))\n",
    "CHECKPOINT_INTERVAL = int(os.getenv('CHECKPOINT_INTERVAL', '50'))\n",
    "\n",
    "# API Rate Limiting\n",
    "MAX_RETRIES = int(os.getenv('MAX_RETRIES', '3'))\n",
    "RETRY_DELAY = int(os.getenv('RETRY_DELAY', '5'))\n",
    "\n",
    "# Checkpoint file\n",
    "CHECKPOINT_FILE = 'data/enrichment_checkpoint.json'\n",
    "RESULTS_FILE = 'data/enriched_metadata_results.csv'\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  - Process Subset: {PROCESS_SUBSET}\")\n",
    "print(f\"  - Subset Size: {SUBSET_SIZE}\")\n",
    "print(f\"  - Database: {DATABASE_URL.split('@')[1] if '@' in DATABASE_URL else 'localhost'}\")\n",
    "print(f\"  - GCS Base URL: {GCS_LOGO_BASE_URL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Fetching\n",
    "\n",
    "def fetch_existing_metadata() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch existing company metadata from Payload CMS database.\n",
    "    \"\"\"\n",
    "    engine = create_engine(CMS_DATABASE_URL)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        stock_code,\n",
    "        company_name,\n",
    "        industry,\n",
    "        market_cap,\n",
    "        listing_date,\n",
    "        address,\n",
    "        summary,\n",
    "        details,\n",
    "        website,\n",
    "        company_logo_link\n",
    "    FROM metadata\n",
    "    WHERE stock_code IS NOT NULL\n",
    "    ORDER BY company_name\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(query, engine)\n",
    "    engine.dispose()\n",
    "    \n",
    "    # Add logo GCS URLs\n",
    "    df['logo_gcs_url'] = df['stock_code'].apply(\n",
    "        lambda code: f\"{GCS_LOGO_BASE_URL}/{code.upper()}.svg\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Fetched {len(df)} companies from Payload CMS\")\n",
    "    return df\n",
    "\n",
    "def load_checkpoint() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load checkpoint data to resume processing.\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "        print(f\"‚úì Loaded checkpoint: {checkpoint['processed_count']} companies processed\")\n",
    "        return checkpoint\n",
    "    return {'processed_count': 0, 'processed_codes': []}\n",
    "\n",
    "def save_checkpoint(checkpoint: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Save checkpoint data for resumption.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(CHECKPOINT_FILE), exist_ok=True)\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "# Fetch data\n",
    "df_metadata = fetch_existing_metadata()\n",
    "checkpoint = load_checkpoint()\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df_metadata.shape}\")\n",
    "print(f\"Columns: {list(df_metadata.columns)}\")\n",
    "df_metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: GPT-5 Schema Definition\n",
    "\n",
    "ENRICHMENT_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"tags\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"description\": \"3-7 specific specialty tags describing the company's focus (e.g., 'lithium mining', 'rare earth magnets', 'renewable energy', 'biotech oncology')\"\n",
    "        },\n",
    "        \"enhanced_summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Comprehensive company overview (500-1000 words) covering business model, market position, key operations, and strategic focus\"\n",
    "        },\n",
    "        \"company_history\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Historical timeline with key milestones, founding story, major acquisitions, pivots, and evolution (300-500 words)\"\n",
    "        },\n",
    "        \"key_people\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"},\n",
    "                    \"role\": {\"type\": \"string\"},\n",
    "                    \"bio\": {\"type\": \"string\", \"description\": \"2-3 sentence biography with relevant experience\"},\n",
    "                    \"linkedin\": {\"type\": \"string\", \"description\": \"LinkedIn profile URL if available\"}\n",
    "                },\n",
    "                \"required\": [\"name\", \"role\"]\n",
    "            },\n",
    "            \"description\": \"Key executives, board members, and senior leadership\"\n",
    "        },\n",
    "        \"financial_reports\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"type\": {\"type\": \"string\", \"enum\": [\"annual_report\", \"quarterly_report\", \"half_year_report\"]},\n",
    "                    \"date\": {\"type\": \"string\", \"description\": \"Report date in YYYY-MM-DD format\"},\n",
    "                    \"url\": {\"type\": \"string\", \"description\": \"Direct URL to the report PDF\"},\n",
    "                    \"title\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"type\", \"url\"]\n",
    "            },\n",
    "            \"description\": \"Links to recent annual and quarterly reports\"\n",
    "        },\n",
    "        \"competitive_advantages\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Unique strengths, market position, competitive moats, and strategic advantages (200-400 words)\"\n",
    "        },\n",
    "        \"risk_factors\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Key business risks including operational, market, regulatory, and financial risks (200-400 words)\"\n",
    "        },\n",
    "        \"recent_developments\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Recent news, announcements, contracts, or developments from the last 12 months (200-400 words)\"\n",
    "        },\n",
    "        \"social_media_links\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"twitter\": {\"type\": \"string\"},\n",
    "                \"linkedin\": {\"type\": \"string\"},\n",
    "                \"facebook\": {\"type\": \"string\"},\n",
    "                \"youtube\": {\"type\": \"string\"}\n",
    "            },\n",
    "            \"description\": \"Official social media profile URLs\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"tags\", \"enhanced_summary\"]\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a financial research analyst specializing in Australian Stock Exchange (ASX) listed companies.\n",
    "Your task is to provide comprehensive, accurate, and well-researched company profiles.\n",
    "\n",
    "Guidelines:\n",
    "1. Use Deep Research to gather accurate, up-to-date information\n",
    "2. Focus on factual, verifiable information\n",
    "3. Provide specific details rather than generic descriptions\n",
    "4. Include relevant industry context and market positioning\n",
    "5. Cite recent developments and concrete examples\n",
    "6. Maintain professional, objective tone\n",
    "7. For tags, use specific, searchable terms that accurately describe the company's specialty\n",
    "8. Ensure all URLs are valid and publicly accessible\n",
    "\n",
    "Return your response as a valid JSON object matching the provided schema.\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úì Schema and prompts defined\")\n",
    "print(f\"  Required fields: {ENRICHMENT_SCHEMA['required']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: GPT-5 Deep Research Function\n",
    "\n",
    "def enrich_company_with_gpt5(company: pd.Series, use_deep_research: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Enrich company metadata using GPT-5 with Deep Research.\n",
    "    \n",
    "    Args:\n",
    "        company: Pandas Series with existing company metadata\n",
    "        use_deep_research: Whether to use Deep Research mode\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with enriched metadata\n",
    "    \"\"\"\n",
    "    stock_code = company['stock_code']\n",
    "    company_name = company['company_name']\n",
    "    \n",
    "    # Prepare context from existing metadata\n",
    "    context = f\"\"\"\n",
    "    Company: {company_name}\n",
    "    ASX Code: {stock_code}\n",
    "    Industry: {company.get('industry', 'N/A')}\n",
    "    Website: {company.get('website', 'N/A')}\n",
    "    Existing Summary: {company.get('summary', 'N/A')}\n",
    "    Address: {company.get('address', 'N/A')}\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Research and provide a comprehensive profile for the following ASX-listed company:\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Please provide detailed, accurate information following the schema. Use Deep Research to find:\n",
    "    - Current company operations and business model\n",
    "    - Recent announcements and developments\n",
    "    - Key leadership team members\n",
    "    - Links to recent annual and quarterly reports\n",
    "    - Company's competitive positioning\n",
    "    - Known risk factors\n",
    "    - Official social media presence\n",
    "    \n",
    "    Focus on factual, verifiable information. For the enhanced_summary, provide a comprehensive\n",
    "    overview that would be suitable for investors and analysts.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Use GPT-5 with structured output\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",  # Will be updated to gpt-5 when available\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                temperature=0.3,\n",
    "                max_tokens=4000\n",
    "            )\n",
    "            \n",
    "            enriched_data = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            # Add metadata\n",
    "            enriched_data['stock_code'] = stock_code\n",
    "            enriched_data['enrichment_date'] = datetime.now().isoformat()\n",
    "            enriched_data['enrichment_status'] = 'completed'\n",
    "            enriched_data['logo_gcs_url'] = company.get('logo_gcs_url')\n",
    "            \n",
    "            return enriched_data\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö† JSON decode error for {stock_code} (attempt {attempt + 1}/{MAX_RETRIES}): {e}\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Error enriching {stock_code} (attempt {attempt + 1}/{MAX_RETRIES}): {e}\")\n",
    "            if \"rate_limit\" in str(e).lower():\n",
    "                time.sleep(RETRY_DELAY * 2)\n",
    "            elif attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "                continue\n",
    "    \n",
    "    # Return minimal data on failure\n",
    "    return {\n",
    "        'stock_code': stock_code,\n",
    "        'enrichment_status': 'failed',\n",
    "        'enrichment_date': datetime.now().isoformat(),\n",
    "        'enrichment_error': 'Failed after maximum retries'\n",
    "    }\n",
    "\n",
    "print(\"‚úì GPT-5 enrichment function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Annual Report Fetcher\n",
    "\n",
    "def fetch_annual_reports(company: pd.Series) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Fetch annual reports from ASX announcements and company website.\n",
    "    \n",
    "    Args:\n",
    "        company: Pandas Series with company metadata\n",
    "    \n",
    "    Returns:\n",
    "        List of report dictionaries with type, date, url, title\n",
    "    \"\"\"\n",
    "    stock_code = company['stock_code']\n",
    "    reports = []\n",
    "    \n",
    "    # Try ASX announcements API\n",
    "    try:\n",
    "        asx_url = f\"https://cdn-api.markitdigital.com/apiman-gateway/ASX/asx-research/1.0/companies/{stock_code}/announcements\"\n",
    "        response = httpx.get(\n",
    "            asx_url,\n",
    "            headers={\"User-Agent\": \"Mozilla/5.0\"},\n",
    "            timeout=10.0,\n",
    "            follow_redirects=True\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Filter for annual and quarterly reports\n",
    "            for announcement in data.get('data', [])[:20]:  # Check last 20 announcements\n",
    "                title = announcement.get('header', '').lower()\n",
    "                if any(keyword in title for keyword in ['annual report', 'full year', 'quarterly', 'half year']):\n",
    "                    report_type = 'annual_report' if 'annual' in title or 'full year' in title else 'quarterly_report'\n",
    "                    if 'half year' in title:\n",
    "                        report_type = 'half_year_report'\n",
    "                    \n",
    "                    reports.append({\n",
    "                        'type': report_type,\n",
    "                        'date': announcement.get('documentDate', ''),\n",
    "                        'url': announcement.get('url', ''),\n",
    "                        'title': announcement.get('header', '')\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö† Could not fetch ASX announcements for {stock_code}: {e}\")\n",
    "    \n",
    "    # Try company website investor relations page\n",
    "    website = company.get('website')\n",
    "    if website and website != 'N/A':\n",
    "        try:\n",
    "            # Common investor relations paths\n",
    "            ir_paths = ['/investors', '/investor-relations', '/investor-centre', '/about/investors']\n",
    "            \n",
    "            for path in ir_paths:\n",
    "                try:\n",
    "                    ir_url = urljoin(website, path)\n",
    "                    response = httpx.get(\n",
    "                        ir_url,\n",
    "                        headers={\"User-Agent\": \"Mozilla/5.0\"},\n",
    "                        timeout=10.0,\n",
    "                        follow_redirects=True\n",
    "                    )\n",
    "                    \n",
    "                    if response.status_code == 200:\n",
    "                        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                        \n",
    "                        # Find PDF links that look like reports\n",
    "                        for link in soup.find_all('a', href=True):\n",
    "                            href = link['href']\n",
    "                            text = link.get_text().lower()\n",
    "                            \n",
    "                            if href.endswith('.pdf') and any(keyword in text for keyword in ['annual', 'report', 'financial']):\n",
    "                                full_url = urljoin(ir_url, href)\n",
    "                                \n",
    "                                # Avoid duplicates\n",
    "                                if not any(r['url'] == full_url for r in reports):\n",
    "                                    reports.append({\n",
    "                                        'type': 'annual_report',\n",
    "                                        'url': full_url,\n",
    "                                        'title': link.get_text().strip()\n",
    "                                    })\n",
    "                        \n",
    "                        break  # Found a working IR page\n",
    "                        \n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö† Could not scrape website for {stock_code}: {e}\")\n",
    "    \n",
    "    return reports[:5]  # Return max 5 most recent reports\n",
    "\n",
    "print(\"‚úì Annual report fetcher defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Resolver Pattern Implementation\n",
    "\n",
    "def resolve_tags(enriched_data: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Validate and clean tags.\n",
    "    \"\"\"\n",
    "    tags = enriched_data.get('tags', [])\n",
    "    \n",
    "    if not isinstance(tags, list):\n",
    "        return []\n",
    "    \n",
    "    # Clean and validate tags\n",
    "    cleaned_tags = []\n",
    "    for tag in tags:\n",
    "        if isinstance(tag, str) and len(tag) > 2 and len(tag) < 50:\n",
    "            cleaned_tags.append(tag.lower().strip())\n",
    "    \n",
    "    return cleaned_tags[:10]  # Max 10 tags\n",
    "\n",
    "def resolve_key_people(enriched_data: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Validate and structure key people data.\n",
    "    \"\"\"\n",
    "    people = enriched_data.get('key_people', [])\n",
    "    \n",
    "    if not isinstance(people, list):\n",
    "        return []\n",
    "    \n",
    "    validated_people = []\n",
    "    for person in people:\n",
    "        if isinstance(person, dict) and 'name' in person and 'role' in person:\n",
    "            validated_people.append({\n",
    "                'name': person['name'],\n",
    "                'role': person['role'],\n",
    "                'bio': person.get('bio', ''),\n",
    "                'linkedin': person.get('linkedin', '')\n",
    "            })\n",
    "    \n",
    "    return validated_people\n",
    "\n",
    "def resolve_financial_reports(enriched_data: Dict[str, Any], company: pd.Series) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Combine GPT-5 results with scraped reports.\n",
    "    \"\"\"\n",
    "    gpt_reports = enriched_data.get('financial_reports', [])\n",
    "    scraped_reports = fetch_annual_reports(company)\n",
    "    \n",
    "    all_reports = []\n",
    "    seen_urls = set()\n",
    "    \n",
    "    # Add both sources, avoiding duplicates\n",
    "    for report in gpt_reports + scraped_reports:\n",
    "        if isinstance(report, dict) and 'url' in report:\n",
    "            url = report['url']\n",
    "            if url and url not in seen_urls:\n",
    "                seen_urls.add(url)\n",
    "                all_reports.append(report)\n",
    "    \n",
    "    return all_reports[:10]  # Max 10 reports\n",
    "\n",
    "def resolve_social_media_links(enriched_data: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Validate social media URLs.\n",
    "    \"\"\"\n",
    "    links = enriched_data.get('social_media_links', {})\n",
    "    \n",
    "    if not isinstance(links, dict):\n",
    "        return {}\n",
    "    \n",
    "    validated_links = {}\n",
    "    url_pattern = re.compile(r'^https?://')\n",
    "    \n",
    "    for platform, url in links.items():\n",
    "        if isinstance(url, str) and url_pattern.match(url):\n",
    "            validated_links[platform] = url\n",
    "    \n",
    "    return validated_links\n",
    "\n",
    "def apply_resolvers(enriched_data: Dict[str, Any], company: pd.Series) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Apply all resolver functions to clean and validate data.\n",
    "    \"\"\"\n",
    "    enriched_data['tags'] = resolve_tags(enriched_data)\n",
    "    enriched_data['key_people'] = resolve_key_people(enriched_data)\n",
    "    enriched_data['financial_reports'] = resolve_financial_reports(enriched_data, company)\n",
    "    enriched_data['social_media_links'] = resolve_social_media_links(enriched_data)\n",
    "    \n",
    "    return enriched_data\n",
    "\n",
    "print(\"‚úì Resolver functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Batch Processing with Subset Support\n",
    "\n",
    "def process_companies(df: pd.DataFrame, checkpoint: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process companies with checkpoint support and progress tracking.\n",
    "    \"\"\"\n",
    "    # Filter companies to process\n",
    "    companies_to_process = df[~df['stock_code'].isin(checkpoint['processed_codes'])]\n",
    "    \n",
    "    if PROCESS_SUBSET:\n",
    "        companies_to_process = companies_to_process.head(SUBSET_SIZE)\n",
    "        print(f\"\\nüìã Processing subset of {len(companies_to_process)} companies\")\n",
    "    else:\n",
    "        print(f\"\\nüìã Processing {len(companies_to_process)} companies (full dataset)\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process with progress bar\n",
    "    for idx, (_, company) in enumerate(tqdm(companies_to_process.iterrows(), total=len(companies_to_process))):\n",
    "        stock_code = company['stock_code']\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nüîç Processing {stock_code} - {company['company_name']}\")\n",
    "            \n",
    "            # Enrich with GPT-5\n",
    "            enriched_data = enrich_company_with_gpt5(company)\n",
    "            \n",
    "            # Apply resolvers\n",
    "            enriched_data = apply_resolvers(enriched_data, company)\n",
    "            \n",
    "            results.append(enriched_data)\n",
    "            \n",
    "            # Update checkpoint\n",
    "            checkpoint['processed_codes'].append(stock_code)\n",
    "            checkpoint['processed_count'] += 1\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if (idx + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "                save_checkpoint(checkpoint)\n",
    "                print(f\"\\nüíæ Checkpoint saved: {checkpoint['processed_count']} companies\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(1)  # Be respectful to APIs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to process {stock_code}: {e}\")\n",
    "            results.append({\n",
    "                'stock_code': stock_code,\n",
    "                'enrichment_status': 'failed',\n",
    "                'enrichment_error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Final checkpoint save\n",
    "    save_checkpoint(checkpoint)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì Batch processing function defined\")\n",
    "print(f\"  Ready to process companies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Data Validation\n",
    "\n",
    "def validate_enriched_data(results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate and generate statistics for enriched data.\n",
    "    \"\"\"\n",
    "    total = len(results)\n",
    "    completed = sum(1 for r in results if r.get('enrichment_status') == 'completed')\n",
    "    failed = sum(1 for r in results if r.get('enrichment_status') == 'failed')\n",
    "    \n",
    "    # Field coverage\n",
    "    field_coverage = {}\n",
    "    fields_to_check = ['tags', 'enhanced_summary', 'company_history', 'key_people', \n",
    "                       'financial_reports', 'competitive_advantages', 'risk_factors', \n",
    "                       'recent_developments', 'social_media_links']\n",
    "    \n",
    "    for field in fields_to_check:\n",
    "        count = sum(1 for r in results if r.get(field) and r.get('enrichment_status') == 'completed')\n",
    "        field_coverage[field] = {\n",
    "            'count': count,\n",
    "            'percentage': (count / completed * 100) if completed > 0 else 0\n",
    "        }\n",
    "    \n",
    "    # Tag statistics\n",
    "    all_tags = []\n",
    "    for r in results:\n",
    "        if r.get('tags'):\n",
    "            all_tags.extend(r['tags'])\n",
    "    \n",
    "    from collections import Counter\n",
    "    tag_freq = Counter(all_tags)\n",
    "    \n",
    "    validation_report = {\n",
    "        'total_processed': total,\n",
    "        'completed': completed,\n",
    "        'failed': failed,\n",
    "        'success_rate': (completed / total * 100) if total > 0 else 0,\n",
    "        'field_coverage': field_coverage,\n",
    "        'unique_tags': len(tag_freq),\n",
    "        'most_common_tags': tag_freq.most_common(20)\n",
    "    }\n",
    "    \n",
    "    return validation_report\n",
    "\n",
    "def print_validation_report(report: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Print formatted validation report.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä ENRICHMENT VALIDATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Completed: {report['completed']}/{report['total_processed']} ({report['success_rate']:.1f}%)\")\n",
    "    print(f\"‚ùå Failed: {report['failed']}\")\n",
    "    \n",
    "    print(\"\\nüìà Field Coverage:\")\n",
    "    for field, stats in report['field_coverage'].items():\n",
    "        print(f\"  {field:.<30} {stats['count']:>3} ({stats['percentage']:>5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è  Unique Tags: {report['unique_tags']}\")\n",
    "    print(\"\\nMost Common Tags:\")\n",
    "    for tag, count in report['most_common_tags'][:10]:\n",
    "        print(f\"  {tag:.<40} {count:>3}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"‚úì Validation functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Database Update\n",
    "\n",
    "def update_database(results: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Update main Postgres database with enriched data using upsert pattern.\n",
    "    \"\"\"\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    \n",
    "    updated_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    print(f\"\\nüíæ Updating database with {len(results)} records...\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        for result in tqdm(results):\n",
    "            stock_code = result.get('stock_code')\n",
    "            \n",
    "            if not stock_code:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Prepare data for upsert\n",
    "                update_data = {\n",
    "                    'tags': result.get('tags', []),\n",
    "                    'enhanced_summary': result.get('enhanced_summary'),\n",
    "                    'company_history': result.get('company_history'),\n",
    "                    'key_people': json.dumps(result.get('key_people', [])),\n",
    "                    'financial_reports': json.dumps(result.get('financial_reports', [])),\n",
    "                    'competitive_advantages': result.get('competitive_advantages'),\n",
    "                    'risk_factors': result.get('risk_factors'),\n",
    "                    'recent_developments': result.get('recent_developments'),\n",
    "                    'social_media_links': json.dumps(result.get('social_media_links', {})),\n",
    "                    'logo_gcs_url': result.get('logo_gcs_url'),\n",
    "                    'enrichment_status': result.get('enrichment_status', 'completed'),\n",
    "                    'enrichment_date': result.get('enrichment_date', datetime.now().isoformat()),\n",
    "                    'enrichment_error': result.get('enrichment_error')\n",
    "                }\n",
    "                \n",
    "                # Upsert query (note: table name has hyphen, needs quotes)\n",
    "                query = text(\"\"\"\n",
    "                    UPDATE \"company-metadata\"\n",
    "                    SET \n",
    "                        tags = :tags,\n",
    "                        enhanced_summary = :enhanced_summary,\n",
    "                        company_history = :company_history,\n",
    "                        key_people = :key_people::jsonb,\n",
    "                        financial_reports = :financial_reports::jsonb,\n",
    "                        competitive_advantages = :competitive_advantages,\n",
    "                        risk_factors = :risk_factors,\n",
    "                        recent_developments = :recent_developments,\n",
    "                        social_media_links = :social_media_links::jsonb,\n",
    "                        logo_gcs_url = :logo_gcs_url,\n",
    "                        enrichment_status = :enrichment_status,\n",
    "                        enrichment_date = :enrichment_date::timestamp,\n",
    "                        enrichment_error = :enrichment_error,\n",
    "                        updated_at = CURRENT_TIMESTAMP\n",
    "                    WHERE stock_code = :stock_code\n",
    "                \"\"\")\n",
    "                \n",
    "                conn.execute(query, {**update_data, 'stock_code': stock_code})\n",
    "                conn.commit()\n",
    "                updated_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Failed to update {stock_code}: {e}\")\n",
    "                failed_count += 1\n",
    "    \n",
    "    engine.dispose()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Database update complete:\")\n",
    "    print(f\"  - Updated: {updated_count}\")\n",
    "    print(f\"  - Failed: {failed_count}\")\n",
    "\n",
    "print(\"‚úì Database update function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Export Results\n",
    "\n",
    "def export_results(results: List[Dict[str, Any]], validation_report: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Export enriched data to CSV and generate summary.\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    os.makedirs(os.path.dirname(RESULTS_FILE), exist_ok=True)\n",
    "    df_results.to_csv(RESULTS_FILE, index=False)\n",
    "    print(f\"\\nüíæ Results exported to: {RESULTS_FILE}\")\n",
    "    \n",
    "    # Save validation report\n",
    "    report_file = RESULTS_FILE.replace('.csv', '_validation_report.json')\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(validation_report, f, indent=2)\n",
    "    print(f\"üìä Validation report saved to: {report_file}\")\n",
    "    \n",
    "    # Generate sample output\n",
    "    if len(df_results) > 0:\n",
    "        print(\"\\nüìÑ Sample Enriched Record:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        sample = df_results[df_results['enrichment_status'] == 'completed'].iloc[0] if len(df_results[df_results['enrichment_status'] == 'completed']) > 0 else df_results.iloc[0]\n",
    "        \n",
    "        print(f\"Stock Code: {sample.get('stock_code')}\")\n",
    "        print(f\"Status: {sample.get('enrichment_status')}\")\n",
    "        print(f\"Tags: {sample.get('tags', [])}\")\n",
    "        print(f\"\\nEnhanced Summary (first 200 chars):\")\n",
    "        summary = sample.get('enhanced_summary', '')\n",
    "        print(summary[:200] + '...' if len(summary) > 200 else summary)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "print(\"‚úì Export functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Execute Pipeline\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ STARTING ENRICHMENT PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Process companies\n",
    "results = process_companies(df_metadata, checkpoint)\n",
    "\n",
    "# Validate results\n",
    "validation_report = validate_enriched_data(results)\n",
    "print_validation_report(validation_report)\n",
    "\n",
    "# Update database\n",
    "update_database(results)\n",
    "\n",
    "# Export results\n",
    "df_results = export_results(results, validation_report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nProcessed: {len(results)} companies\")\n",
    "print(f\"Success Rate: {validation_report['success_rate']:.1f}%\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(\"  1. Review the validation report\")\n",
    "print(\"  2. Check sample records for quality\")\n",
    "print(\"  3. If satisfied, run with PROCESS_SUBSET=False for full dataset\")\n",
    "print(\"  4. Monitor API costs and adjust as needed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Optional - View Results\n",
    "\n",
    "# Display results summary\n",
    "if 'df_results' in locals():\n",
    "    print(\"\\nüìä Results Summary:\")\n",
    "    print(f\"Total records: {len(df_results)}\")\n",
    "    print(f\"\\nStatus distribution:\")\n",
    "    print(df_results['enrichment_status'].value_counts())\n",
    "    \n",
    "    # Show companies with most tags\n",
    "    if 'tags' in df_results.columns:\n",
    "        df_results['tag_count'] = df_results['tags'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        print(\"\\nüè∑Ô∏è  Companies with most tags:\")\n",
    "        print(df_results[['stock_code', 'tag_count', 'tags']].sort_values('tag_count', ascending=False).head(10))\n",
    "    \n",
    "    # Display full DataFrame\n",
    "    df_results.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
