.PHONY: help install download populate populate-skip-download append dry-run clean verify enrich enrich-logos enrich-full

# Default database URL for local development
# Note: Local docker-compose uses admin/password on port 5438
DATABASE_URL ?= postgresql://admin:password@localhost:5438/shorts

# Default target
help:
	@echo "ðŸ“Š Short Position Data Population & Enrichment"
	@echo ""
	@echo "Available commands:"
	@echo ""
	@echo "ðŸ“¥ DATA POPULATION:"
	@echo "  make install                 - Install Python dependencies"
	@echo "  make download                - Download CSV files from ASIC (10-20 min)"
	@echo "  make populate                - Download + process + load to DB (30-60 min)"
	@echo "  make populate-skip-download  - Process existing CSVs + load to DB (15-20 min)"
	@echo "  make append                  - Append new data to existing table"
	@echo "  make dry-run                 - Test processing without writing to DB"
	@echo "  make verify                  - Verify data in database"
	@echo "  make clean                   - Remove downloaded CSV files"
	@echo ""
	@echo "ðŸŽ¨ BATCH ENRICHMENT:"
	@echo "  make enrich                  - Run batch enrichment (metadata + logos)"
	@echo "  make enrich-subset N=10      - Run enrichment on N companies (default: 10)"
	@echo "  make enrich-full             - Run enrichment on ALL companies"
	@echo "  make notebook                - Start Jupyter notebook server"
	@echo ""
	@echo "Database:"
	@echo "  Defaults to: postgresql://admin:password@localhost:5438/shorts"
	@echo "  Override with: export DATABASE_URL='postgresql://...'"
	@echo ""
	@echo "Example usage:"
	@echo "  make install"
	@echo "  make populate-skip-download"
	@echo "  make enrich-subset N=5       # Test on 5 companies"
	@echo ""
	@echo "  # Or with custom database:"
	@echo "  export DATABASE_URL='postgresql://user:pass@host:5432/db'"
	@echo "  make enrich"

# Install Python dependencies
install:
	@echo "ðŸ“¦ Installing Python dependencies..."
	pip install -r requirements.txt
	@echo "âœ… Dependencies installed"

# Download CSV files from ASIC
download:
	@echo "ðŸ“¥ Downloading short position data from ASIC..."
	@echo "âš ï¸  This will download ~3,500 CSV files (~1GB)"
	@echo "â±ï¸  Estimated time: 10-20 minutes"
	@echo ""
	@read -p "Continue? [y/N] " -n 1 -r; \
	echo; \
	if [[ $$REPLY =~ ^[Yy]$$ ]]; then \
		cd ../scripts && npm install && npx ts-node sync-short-data.ts; \
	else \
		echo "âŒ Cancelled"; \
		exit 1; \
	fi

# Full population: download + process + load
populate:
	@echo "ðŸš€ Full population: download + process + load"
	@echo "ðŸ“Š Database: $(DATABASE_URL)"
	@echo "â±ï¸  Estimated time: 30-60 minutes"
	@echo ""
	DATABASE_URL=$(DATABASE_URL) python3 populate_shorts_from_csv.py

# Process existing CSVs and load to database
populate-skip-download:
	@echo "ðŸš€ Processing existing CSV files and loading to database..."
	@echo "ðŸ“Š Database: $(DATABASE_URL)"
	@echo "â±ï¸  Estimated time: 15-20 minutes"
	@echo ""
	DATABASE_URL=$(DATABASE_URL) python3 populate_shorts_from_csv.py --skip-download

# Append new data to existing table
append:
	@echo "âž• Appending new data to existing shorts table..."
	@echo "ðŸ“Š Database: $(DATABASE_URL)"
	DATABASE_URL=$(DATABASE_URL) python3 populate_shorts_from_csv.py --skip-download --append

# Dry run - process but don't write to database
dry-run:
	@echo "ðŸ§ª Dry run - processing data without database write..."
	python3 populate_shorts_from_csv.py --skip-download --dry-run

# Verify data in database
verify:
	@echo "âœ… Verifying shorts data in database..."
	@echo "ðŸ“Š Database: $(DATABASE_URL)"
	@echo ""
	@echo "ðŸ“Š Record counts:"
	@psql "$(DATABASE_URL)" -c "SELECT COUNT(*) as total_records FROM shorts;"
	@echo ""
	@echo "ðŸ“Š Unique stocks:"
	@psql "$(DATABASE_URL)" -c "SELECT COUNT(DISTINCT \"PRODUCT_CODE\") as unique_stocks FROM shorts;"
	@echo ""
	@echo "ðŸ“… Date range:"
	@psql "$(DATABASE_URL)" -c "SELECT MIN(\"DATE\")::date as earliest, MAX(\"DATE\")::date as latest FROM shorts;"
	@echo ""
	@echo "ðŸ“ˆ Top 10 stocks by record count:"
	@psql "$(DATABASE_URL)" -c "SELECT \"PRODUCT_CODE\", COUNT(*) as records FROM shorts GROUP BY \"PRODUCT_CODE\" ORDER BY records DESC LIMIT 10;"

# Clean downloaded CSV files
clean:
	@echo "ðŸ§¹ Cleaning downloaded CSV files..."
	@if [ -d "data/shorts" ]; then \
		rm -rf data/shorts/*.csv; \
		echo "âœ… Cleaned $(shell ls -1 data/shorts/*.csv 2>/dev/null | wc -l) CSV files"; \
	else \
		echo "âœ… No CSV files to clean"; \
	fi

# Quick start with sample data (no download required)
sample:
	@echo "ðŸš€ Loading sample data (CBA, BHP, RMD, RMX)..."
	@echo "ðŸ“Š Database: $(DATABASE_URL)"
	@psql "$(DATABASE_URL)" -f sql/init-db.sql
	@echo "âœ… Sample data loaded"

# Check if CSV files exist
check-csvs:
	@echo "ðŸ“ Checking for CSV files..."
	@if [ -d "data/shorts" ]; then \
		count=$$(ls -1 data/shorts/*.csv 2>/dev/null | wc -l | tr -d ' '); \
		if [ "$$count" -gt 0 ]; then \
			echo "âœ… Found $$count CSV files in data/shorts/"; \
			size=$$(du -sh data/shorts 2>/dev/null | cut -f1); \
			echo "ðŸ“¦ Total size: $$size"; \
		else \
			echo "âš ï¸  No CSV files found in data/shorts/"; \
			echo "   Run 'make download' to fetch them"; \
		fi; \
	else \
		echo "âš ï¸  Directory data/shorts/ does not exist"; \
		echo "   Run 'make download' to create it and fetch CSV files"; \
	fi

# Show status of everything
status:
	@echo "ðŸ“Š SHORT POSITION DATA STATUS"
	@echo "========================================"
	@echo ""
	@echo "1ï¸âƒ£  Python Dependencies:"
	@if python3 -c "import httpx, tqdm, pandas, dask, chardet, sqlalchemy, psycopg2" 2>/dev/null; then \
		echo "   âœ… All dependencies installed"; \
	else \
		echo "   âŒ Missing dependencies (run 'make install')"; \
	fi
	@echo ""
	@echo "2ï¸âƒ£  CSV Files:"
	@if [ -d "data/shorts" ]; then \
		count=$$(ls -1 data/shorts/*.csv 2>/dev/null | wc -l | tr -d ' '); \
		if [ "$$count" -gt 0 ]; then \
			echo "   âœ… Found $$count CSV files"; \
		else \
			echo "   âš ï¸  No CSV files (run 'make download')"; \
		fi; \
	else \
		echo "   âš ï¸  No CSV files (run 'make download')"; \
	fi
	@echo ""
	@echo "3ï¸âƒ£  Database:"
	@echo "   ðŸ“Š URL: $(DATABASE_URL)"
	@if psql "$(DATABASE_URL)" -c "SELECT 1 FROM shorts LIMIT 1" >/dev/null 2>&1; then \
		count=$$(psql "$(DATABASE_URL)" -tAc "SELECT COUNT(*) FROM shorts"); \
		echo "   âœ… Connected - $$count records in shorts table"; \
	else \
		echo "   âš ï¸  Cannot query shorts table (may be empty or not connected)"; \
	fi
	@echo ""
	@echo "========================================"
	@echo "Run 'make help' for available commands"

# ============================================
# BATCH ENRICHMENT TARGETS
# ============================================

# Default subset size
N ?= 10

# Run batch enrichment notebook (subset mode for testing)
enrich:
	@echo "ðŸŽ¨ Running batch enrichment (subset mode: $(N) companies)..."
	@echo "ðŸ“Š Database: $(DATABASE_URL)"
	@echo ""
	DATABASE_URL=$(DATABASE_URL) PROCESS_SUBSET=True SUBSET_SIZE=$(N) jupyter nbconvert --to notebook --execute batch-enrichment.ipynb --output batch-enrichment-output.ipynb
	@echo "âœ… Enrichment complete - check batch-enrichment-output.ipynb for results"

# Run enrichment on N companies (alias for enrich)
enrich-subset:
	@echo "ðŸŽ¨ Running batch enrichment on $(N) companies..."
	@echo "ðŸ“Š Database: $(DATABASE_URL)"
	DATABASE_URL=$(DATABASE_URL) PROCESS_SUBSET=True SUBSET_SIZE=$(N) jupyter nbconvert --to notebook --execute batch-enrichment.ipynb --output batch-enrichment-output.ipynb
	@echo "âœ… Enrichment complete"

# Run enrichment on ALL companies (caution: can take hours and cost $$$)
enrich-full:
	@echo "ðŸš¨ FULL ENRICHMENT MODE"
	@echo "ðŸ“Š Database: $(DATABASE_URL)"
	@echo "âš ï¸  This will process ALL companies in the database!"
	@echo "âš ï¸  This can take several hours and may incur significant API costs."
	@echo ""
	@read -p "Are you sure you want to continue? [y/N] " -n 1 -r; \
	echo; \
	if [[ $$REPLY =~ ^[Yy]$$ ]]; then \
		DATABASE_URL=$(DATABASE_URL) PROCESS_SUBSET=False jupyter nbconvert --to notebook --execute batch-enrichment.ipynb --output batch-enrichment-output.ipynb; \
		echo "âœ… Full enrichment complete"; \
	else \
		echo "âŒ Cancelled"; \
	fi

# Start Jupyter notebook server
notebook:
	@echo "ðŸ““ Starting Jupyter notebook server..."
	@echo "ðŸ“Š Database: $(DATABASE_URL)"
	@echo ""
	DATABASE_URL=$(DATABASE_URL) jupyter notebook

# Install logo processing dependencies (optional, for full logo processing)
install-logo-processing:
	@echo "ðŸ“¦ Installing logo processing dependencies..."
	@echo "âš ï¸  This includes heavy ML dependencies (rembg, easyocr)"
	@echo ""
	pip install rembg>=2.0.50 onnxruntime>=1.15.0 easyocr>=1.7.1 opencv-contrib-python>=4.8.0 'numpy<2.0.0'
	@echo "âœ… Logo processing dependencies installed"

# Check enrichment status
enrich-status:
	@echo "ðŸ“Š ENRICHMENT STATUS"
	@echo "========================================"
	@echo ""
	@echo "1ï¸âƒ£  Companies by enrichment status:"
	@psql "$(DATABASE_URL)" -c "SELECT enrichment_status, COUNT(*) FROM \"company-metadata\" GROUP BY enrichment_status ORDER BY COUNT(*) DESC;"
	@echo ""
	@echo "2ï¸âƒ£  Companies with logos:"
	@psql "$(DATABASE_URL)" -c "SELECT COUNT(*) as with_logo FROM \"company-metadata\" WHERE logo_gcs_url IS NOT NULL AND logo_gcs_url != '';"
	@echo ""
	@echo "3ï¸âƒ£  Companies needing enrichment:"
	@psql "$(DATABASE_URL)" -c "SELECT COUNT(*) as needs_enrichment FROM \"company-metadata\" WHERE enrichment_status IS NULL OR enrichment_status = '' OR enrichment_status = 'pending';"
	@echo ""
	@echo "========================================"

# Clear enrichment checkpoint (to restart from scratch)
enrich-reset:
	@echo "ðŸ”„ Resetting enrichment checkpoint..."
	@rm -f data/batch_enrichment_checkpoint.json
	@echo "âœ… Checkpoint cleared - enrichment will start from the beginning"

