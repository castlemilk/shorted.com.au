{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch Stock Enrichment Pipeline\n",
        "\n",
        "Comprehensive enrichment pipeline for ASX companies including:\n",
        "- **Company Metadata** - Tags, summaries, key people, history, risks\n",
        "- **Logo Discovery** - Intelligent website scraping to find company logos\n",
        "- **Logo Processing** - Background removal, icon extraction, upscaling\n",
        "- **GCS Upload** - Store processed logos in Google Cloud Storage\n",
        "\n",
        "## Cost Optimization\n",
        "Running locally is more cost-effective than using the Cloud Run enrichment service:\n",
        "- No per-request Cloud Run charges\n",
        "- No Pub/Sub message charges\n",
        "- Can batch process during off-peak hours\n",
        "- Direct database updates without API overhead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Dependencies and Setup\n",
        "import httpx\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "import json\n",
        "from sqlalchemy import create_engine, text\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import time\n",
        "from typing import Dict, List, Optional, Any, Tuple\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse, urlunparse\n",
        "from dataclasses import dataclass, field\n",
        "from io import BytesIO\n",
        "import subprocess\n",
        "import tempfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"‚úì Dependencies loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Configuration\n",
        "\n",
        "# OpenAI Configuration\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError('OPENAI_API_KEY environment variable is required')\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Database Configuration\n",
        "DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://admin:password@localhost:5438/shorts')\n",
        "\n",
        "# GCS Configuration\n",
        "GCS_BUCKET = os.getenv('GCS_LOGO_BUCKET', 'shorted-company-logos')\n",
        "GCS_LOGO_BASE_URL = f'https://storage.googleapis.com/{GCS_BUCKET}/logos'\n",
        "\n",
        "# Processing Configuration\n",
        "PROCESS_SUBSET = os.getenv('PROCESS_SUBSET', 'True').lower() == 'true'\n",
        "SUBSET_SIZE = int(os.getenv('SUBSET_SIZE', '10'))\n",
        "BATCH_SIZE = int(os.getenv('BATCH_SIZE', '50'))\n",
        "CHECKPOINT_INTERVAL = int(os.getenv('CHECKPOINT_INTERVAL', '10'))\n",
        "\n",
        "# Logo Processing Configuration\n",
        "LOGO_PROCESSOR_PATH = Path('../services/enrichment-processor/logo_processor.py').resolve()\n",
        "SVG_TEXT_REMOVER_PATH = Path('../services/enrichment-processor/svg_text_remover.py').resolve()\n",
        "SVG_RENDERER_PATH = Path('../services/enrichment-processor/svg_renderer.py').resolve()\n",
        "ENRICHMENT_PROCESSOR_DIR = Path('../services/enrichment-processor').resolve()\n",
        "\n",
        "# API Rate Limiting\n",
        "MAX_RETRIES = int(os.getenv('MAX_RETRIES', '3'))\n",
        "RETRY_DELAY = int(os.getenv('RETRY_DELAY', '5'))\n",
        "\n",
        "# Checkpoint files\n",
        "CHECKPOINT_FILE = 'data/batch_enrichment_checkpoint.json'\n",
        "RESULTS_FILE = 'data/batch_enrichment_results.csv'\n",
        "\n",
        "print(f\"Configuration loaded:\")\n",
        "print(f\"  - Process Subset: {PROCESS_SUBSET}\")\n",
        "print(f\"  - Subset Size: {SUBSET_SIZE}\")\n",
        "print(f\"  - Database: {DATABASE_URL.split('@')[1] if '@' in DATABASE_URL else 'localhost'}\")\n",
        "print(f\"  - GCS Bucket: {GCS_BUCKET}\")\n",
        "print(f\"  - Logo Processor: {LOGO_PROCESSOR_PATH.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logo Discovery\n",
        "\n",
        "Intelligent logo scraper that crawls company websites to find logo candidates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Logo Discovery Classes\n",
        "\n",
        "@dataclass\n",
        "class LogoCandidate:\n",
        "    \"\"\"Represents a potential logo found on a website.\"\"\"\n",
        "    url: str\n",
        "    format: str = 'unknown'  # svg, png, jpeg, gif, webp\n",
        "    source: str = 'unknown'  # img_tag, og_image, favicon, linked_svg, etc.\n",
        "    score: float = 0.0\n",
        "    width: int = 0\n",
        "    height: int = 0\n",
        "    alt: str = ''\n",
        "    found_on_page: str = ''\n",
        "\n",
        "@dataclass\n",
        "class DiscoveredLogo:\n",
        "    \"\"\"Contains information about a discovered and downloaded logo.\"\"\"\n",
        "    source_url: str\n",
        "    image_data: bytes\n",
        "    format: str\n",
        "    width: int = 0\n",
        "    height: int = 0\n",
        "    quality_score: float = 0.0\n",
        "    is_svg: bool = False\n",
        "    svg_data: bytes = field(default_factory=bytes)\n",
        "\n",
        "class LogoScraper:\n",
        "    \"\"\"Intelligent logo scraper that crawls websites to find logo candidates.\"\"\"\n",
        "    \n",
        "    LOGO_KEYWORDS = ['logo', 'brand', 'emblem', 'mark', 'icon']\n",
        "    BRAND_KEYWORDS = ['brand', 'media', 'press', 'logo', 'asset', 'download', 'kit']\n",
        "    COMMON_BRAND_PATHS = [\n",
        "        '/brand', '/brand-assets', '/media', '/media-kit',\n",
        "        '/press', '/press-kit', '/about', '/about-us', '/assets', '/logos'\n",
        "    ]\n",
        "    \n",
        "    def __init__(self, company_name: str = '', max_pages: int = 5):\n",
        "        self.company_name = company_name.lower()\n",
        "        self.max_pages = max_pages\n",
        "        self.client = httpx.Client(\n",
        "            timeout=30.0,\n",
        "            follow_redirects=True,\n",
        "            headers={\n",
        "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
        "                'Accept-Language': 'en-US,en;q=0.9',\n",
        "            }\n",
        "        )\n",
        "    \n",
        "    def scrape_logos(self, website_url: str) -> List[LogoCandidate]:\n",
        "        \"\"\"Scrape the website for logo candidates.\"\"\"\n",
        "        candidates = []\n",
        "        \n",
        "        # Normalize URL\n",
        "        base_url = self._normalize_base_url(website_url)\n",
        "        if not base_url:\n",
        "            return candidates\n",
        "        \n",
        "        # Phase 1: Scan homepage\n",
        "        homepage_candidates, brand_links = self._scan_page(base_url, base_url)\n",
        "        candidates.extend(homepage_candidates)\n",
        "        \n",
        "        # Phase 2: Add common brand paths\n",
        "        for path in self.COMMON_BRAND_PATHS:\n",
        "            page_url = base_url + path\n",
        "            if page_url not in brand_links:\n",
        "                brand_links.append(page_url)\n",
        "        \n",
        "        # Phase 3: Crawl brand/media pages\n",
        "        crawled = {base_url}\n",
        "        for link in brand_links[:self.max_pages]:\n",
        "            if link in crawled:\n",
        "                continue\n",
        "            crawled.add(link)\n",
        "            page_candidates, _ = self._scan_page(link, base_url)\n",
        "            candidates.extend(page_candidates)\n",
        "        \n",
        "        # Score all candidates\n",
        "        for c in candidates:\n",
        "            c.score = self._score_candidate(c)\n",
        "        \n",
        "        # Sort by score (descending)\n",
        "        candidates.sort(key=lambda x: x.score, reverse=True)\n",
        "        \n",
        "        return candidates\n",
        "    \n",
        "    def _normalize_base_url(self, raw_url: str) -> Optional[str]:\n",
        "        \"\"\"Normalize URL to base URL.\"\"\"\n",
        "        if not raw_url.startswith(('http://', 'https://')):\n",
        "            raw_url = 'https://' + raw_url\n",
        "        try:\n",
        "            parsed = urlparse(raw_url)\n",
        "            return f\"{parsed.scheme}://{parsed.netloc}\"\n",
        "        except Exception:\n",
        "            return None\n",
        "    \n",
        "    def _scan_page(self, page_url: str, base_url: str) -> Tuple[List[LogoCandidate], List[str]]:\n",
        "        \"\"\"Extract logo candidates and brand links from a page.\"\"\"\n",
        "        candidates = []\n",
        "        brand_links = []\n",
        "        \n",
        "        try:\n",
        "            response = self.client.get(page_url)\n",
        "            if response.status_code != 200:\n",
        "                return candidates, brand_links\n",
        "            \n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            \n",
        "            # Extract different types of logo candidates\n",
        "            candidates.extend(self._extract_img_tags(soup, page_url, base_url))\n",
        "            candidates.extend(self._extract_og_image(soup, page_url, base_url))\n",
        "            candidates.extend(self._extract_favicons(soup, page_url, base_url))\n",
        "            candidates.extend(self._extract_linked_svgs(soup, page_url, base_url))\n",
        "            \n",
        "            # Extract brand links\n",
        "            brand_links = self._extract_brand_links(soup, base_url)\n",
        "            \n",
        "        except Exception as e:\n",
        "            pass\n",
        "        \n",
        "        return candidates, brand_links\n",
        "    \n",
        "    def _extract_img_tags(self, soup: BeautifulSoup, page_url: str, base_url: str) -> List[LogoCandidate]:\n",
        "        \"\"\"Extract logo candidates from img tags.\"\"\"\n",
        "        candidates = []\n",
        "        for img in soup.find_all('img'):\n",
        "            src = img.get('src') or img.get('data-src') or ''\n",
        "            if not src:\n",
        "                srcset = img.get('srcset', '')\n",
        "                if srcset:\n",
        "                    src = srcset.split(',')[0].split()[0].strip()\n",
        "            if not src or src.startswith('data:'):\n",
        "                continue\n",
        "            \n",
        "            absolute_url = urljoin(base_url, src)\n",
        "            format_ = self._detect_format(absolute_url)\n",
        "            alt = img.get('alt', '')\n",
        "            class_ = ' '.join(img.get('class', []))\n",
        "            id_ = img.get('id', '')\n",
        "            \n",
        "            # Check if in header/nav\n",
        "            in_header = img.find_parent(['header', 'nav']) is not None\n",
        "            source = 'img_tag_header' if in_header else 'img_tag'\n",
        "            if format_ == 'svg':\n",
        "                source = 'img_svg_header' if in_header else 'img_svg'\n",
        "            \n",
        "            candidate = LogoCandidate(\n",
        "                url=absolute_url,\n",
        "                format=format_,\n",
        "                source=source,\n",
        "                width=self._parse_int(img.get('width', '0')),\n",
        "                height=self._parse_int(img.get('height', '0')),\n",
        "                alt=alt,\n",
        "                found_on_page=page_url,\n",
        "            )\n",
        "            \n",
        "            # Boost if logo-related\n",
        "            if self._contains_logo_keyword(src) or self._contains_logo_keyword(alt) or \\\n",
        "               self._contains_logo_keyword(class_) or self._contains_logo_keyword(id_):\n",
        "                candidate.score += 20\n",
        "            \n",
        "            candidates.append(candidate)\n",
        "        return candidates\n",
        "    \n",
        "    def _extract_og_image(self, soup: BeautifulSoup, page_url: str, base_url: str) -> List[LogoCandidate]:\n",
        "        \"\"\"Extract logo candidates from og:image meta tags.\"\"\"\n",
        "        candidates = []\n",
        "        for meta in soup.find_all('meta', property='og:image'):\n",
        "            content = meta.get('content', '')\n",
        "            if not content:\n",
        "                continue\n",
        "            absolute_url = urljoin(base_url, content)\n",
        "            candidates.append(LogoCandidate(\n",
        "                url=absolute_url,\n",
        "                format=self._detect_format(absolute_url),\n",
        "                source='og_image',\n",
        "                found_on_page=page_url,\n",
        "            ))\n",
        "        return candidates\n",
        "    \n",
        "    def _extract_favicons(self, soup: BeautifulSoup, page_url: str, base_url: str) -> List[LogoCandidate]:\n",
        "        \"\"\"Extract logo candidates from favicon links.\"\"\"\n",
        "        candidates = []\n",
        "        for link in soup.find_all('link', rel=lambda x: x and any(r in x for r in ['icon', 'apple-touch-icon'])):\n",
        "            href = link.get('href', '')\n",
        "            if not href:\n",
        "                continue\n",
        "            absolute_url = urljoin(base_url, href)\n",
        "            rel = ' '.join(link.get('rel', []))\n",
        "            source = 'apple_touch_icon' if 'apple-touch-icon' in rel else 'favicon'\n",
        "            \n",
        "            sizes = link.get('sizes', '')\n",
        "            width, height = 0, 0\n",
        "            if 'x' in sizes:\n",
        "                parts = sizes.split('x')\n",
        "                width = self._parse_int(parts[0])\n",
        "                height = self._parse_int(parts[1]) if len(parts) > 1 else width\n",
        "            \n",
        "            candidates.append(LogoCandidate(\n",
        "                url=absolute_url,\n",
        "                format=self._detect_format(absolute_url),\n",
        "                source=source,\n",
        "                width=width,\n",
        "                height=height,\n",
        "                found_on_page=page_url,\n",
        "            ))\n",
        "        return candidates\n",
        "    \n",
        "    def _extract_linked_svgs(self, soup: BeautifulSoup, page_url: str, base_url: str) -> List[LogoCandidate]:\n",
        "        \"\"\"Extract logo candidates from linked SVG files.\"\"\"\n",
        "        candidates = []\n",
        "        for a in soup.find_all('a', href=True):\n",
        "            href = a.get('href', '').lower()\n",
        "            if href.endswith('.svg') or 'logo' in href or 'brand' in href:\n",
        "                absolute_url = urljoin(base_url, a.get('href', ''))\n",
        "                if absolute_url.lower().endswith('.svg'):\n",
        "                    candidates.append(LogoCandidate(\n",
        "                        url=absolute_url,\n",
        "                        format='svg',\n",
        "                        source='linked_svg',\n",
        "                        found_on_page=page_url,\n",
        "                    ))\n",
        "        return candidates\n",
        "    \n",
        "    def _extract_brand_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n",
        "        \"\"\"Extract links to brand/media pages.\"\"\"\n",
        "        brand_links = []\n",
        "        for a in soup.find_all('a', href=True):\n",
        "            href = a.get('href', '').lower()\n",
        "            text = a.get_text().lower()\n",
        "            if any(kw in href or kw in text for kw in self.BRAND_KEYWORDS):\n",
        "                absolute_url = urljoin(base_url, a.get('href', ''))\n",
        "                if not absolute_url.startswith(('mailto:', 'tel:')):\n",
        "                    brand_links.append(absolute_url)\n",
        "        return brand_links\n",
        "    \n",
        "    def _score_candidate(self, c: LogoCandidate) -> float:\n",
        "        \"\"\"Calculate score for a logo candidate.\"\"\"\n",
        "        score = 0.0\n",
        "        \n",
        "        # Format scoring (SVG is best)\n",
        "        format_scores = {'svg': 100, 'png': 50, 'webp': 45, 'jpeg': 30, 'jpg': 30, 'gif': 20}\n",
        "        score += format_scores.get(c.format, 0)\n",
        "        \n",
        "        # Size scoring\n",
        "        if c.width > 0:\n",
        "            score += c.width / 100.0\n",
        "            if c.width >= 256:\n",
        "                score += 10\n",
        "            if c.width >= 512:\n",
        "                score += 10\n",
        "        \n",
        "        # Source scoring\n",
        "        source_scores = {\n",
        "            'img_svg_header': 25, 'img_tag_header': 20, 'linked_svg': 15,\n",
        "            'img_svg': 12, 'apple_touch_icon': 12, 'og_image': 10,\n",
        "            'img_tag': 5, 'favicon': 3\n",
        "        }\n",
        "        score += source_scores.get(c.source, 0)\n",
        "        \n",
        "        # Name relevance\n",
        "        url_lower = c.url.lower()\n",
        "        alt_lower = c.alt.lower()\n",
        "        if 'logo' in url_lower or 'logo' in alt_lower:\n",
        "            score += 20\n",
        "        if 'brand' in url_lower or 'brand' in alt_lower:\n",
        "            score += 15\n",
        "        if self.company_name and (self.company_name in url_lower or self.company_name in alt_lower):\n",
        "            score += 10\n",
        "        \n",
        "        # Penalize tiny images\n",
        "        if 0 < c.width < 32:\n",
        "            score -= 20\n",
        "        \n",
        "        return score + c.score  # Add any pre-existing score\n",
        "    \n",
        "    def _detect_format(self, url: str) -> str:\n",
        "        \"\"\"Detect image format from URL.\"\"\"\n",
        "        url_lower = url.lower()\n",
        "        parsed = urlparse(url_lower)\n",
        "        path = parsed.path\n",
        "        \n",
        "        if path.endswith('.svg'):\n",
        "            return 'svg'\n",
        "        elif path.endswith('.png'):\n",
        "            return 'png'\n",
        "        elif path.endswith(('.jpg', '.jpeg')):\n",
        "            return 'jpeg'\n",
        "        elif path.endswith('.gif'):\n",
        "            return 'gif'\n",
        "        elif path.endswith('.webp'):\n",
        "            return 'webp'\n",
        "        elif path.endswith('.ico'):\n",
        "            return 'ico'\n",
        "        return 'unknown'\n",
        "    \n",
        "    def _contains_logo_keyword(self, s: str) -> bool:\n",
        "        \"\"\"Check if string contains logo-related keyword.\"\"\"\n",
        "        s_lower = s.lower()\n",
        "        return any(kw in s_lower for kw in self.LOGO_KEYWORDS)\n",
        "    \n",
        "    def _parse_int(self, val: str) -> int:\n",
        "        \"\"\"Parse integer from string.\"\"\"\n",
        "        try:\n",
        "            return int(val.replace('px', '').strip())\n",
        "        except (ValueError, AttributeError):\n",
        "            return 0\n",
        "\n",
        "print(\"‚úì Logo discovery classes defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Logo Discoverer\n",
        "\n",
        "class LogoDiscoverer:\n",
        "    \"\"\"Discovers and downloads company logos.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.client = httpx.Client(\n",
        "            timeout=30.0,\n",
        "            follow_redirects=True,\n",
        "            headers={\n",
        "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',\n",
        "                'Accept': 'image/svg+xml,image/*,*/*;q=0.8',\n",
        "            }\n",
        "        )\n",
        "    \n",
        "    def discover_logo(self, website: str, company_name: str, stock_code: str) -> Optional[DiscoveredLogo]:\n",
        "        \"\"\"Discover and download the best logo for a company.\"\"\"\n",
        "        if not website or not website.strip():\n",
        "            return None\n",
        "        \n",
        "        scraper = LogoScraper(company_name=company_name)\n",
        "        candidates = scraper.scrape_logos(website)\n",
        "        \n",
        "        if not candidates:\n",
        "            print(f\"  No logo candidates found for {stock_code}\")\n",
        "            return None\n",
        "        \n",
        "        print(f\"  Found {len(candidates)} logo candidates for {stock_code}\")\n",
        "        \n",
        "        # Try candidates in order of score\n",
        "        for candidate in candidates:\n",
        "            if candidate.url.startswith('data:'):\n",
        "                continue\n",
        "            \n",
        "            logo = self._fetch_logo(candidate)\n",
        "            if logo:\n",
        "                print(f\"  ‚úì Downloaded logo: {candidate.url[:80]}... (format: {logo.format}, score: {candidate.score:.1f})\")\n",
        "                return logo\n",
        "        \n",
        "        print(f\"  No valid logo could be fetched for {stock_code}\")\n",
        "        return None\n",
        "    \n",
        "    def _fetch_logo(self, candidate: LogoCandidate) -> Optional[DiscoveredLogo]:\n",
        "        \"\"\"Download and validate a logo from a candidate.\"\"\"\n",
        "        try:\n",
        "            response = self.client.get(candidate.url)\n",
        "            if response.status_code != 200:\n",
        "                return None\n",
        "            \n",
        "            data = response.content\n",
        "            if not data:\n",
        "                return None\n",
        "            \n",
        "            # Detect format from content-type\n",
        "            format_ = candidate.format\n",
        "            content_type = response.headers.get('content-type', '').lower()\n",
        "            if format_ == 'unknown':\n",
        "                format_ = self._detect_format_from_content_type(content_type)\n",
        "            \n",
        "            # Validate it's an image\n",
        "            if not self._is_valid_image(format_, content_type):\n",
        "                return None\n",
        "            \n",
        "            logo = DiscoveredLogo(\n",
        "                source_url=candidate.url,\n",
        "                image_data=data,\n",
        "                format=format_,\n",
        "                width=candidate.width,\n",
        "                height=candidate.height,\n",
        "                quality_score=candidate.score,\n",
        "                is_svg=(format_ == 'svg'),\n",
        "            )\n",
        "            \n",
        "            if format_ == 'svg':\n",
        "                logo.svg_data = data\n",
        "            \n",
        "            return logo\n",
        "        \n",
        "        except Exception as e:\n",
        "            return None\n",
        "    \n",
        "    def _detect_format_from_content_type(self, content_type: str) -> str:\n",
        "        \"\"\"Detect format from HTTP Content-Type header.\"\"\"\n",
        "        if 'svg' in content_type:\n",
        "            return 'svg'\n",
        "        elif 'png' in content_type:\n",
        "            return 'png'\n",
        "        elif 'jpeg' in content_type or 'jpg' in content_type:\n",
        "            return 'jpeg'\n",
        "        elif 'webp' in content_type:\n",
        "            return 'webp'\n",
        "        elif 'gif' in content_type:\n",
        "            return 'gif'\n",
        "        elif 'ico' in content_type:\n",
        "            return 'ico'\n",
        "        return 'unknown'\n",
        "    \n",
        "    def _is_valid_image(self, format_: str, content_type: str) -> bool:\n",
        "        \"\"\"Check if format/content-type indicates a valid image.\"\"\"\n",
        "        valid_formats = {'svg', 'png', 'jpeg', 'jpg', 'webp', 'gif', 'ico'}\n",
        "        if format_ in valid_formats:\n",
        "            return True\n",
        "        if content_type.startswith('image/') or 'svg' in content_type:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "print(\"‚úì Logo discoverer defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Logo Processing Functions\n",
        "\n",
        "def process_logo(logo: DiscoveredLogo, stock_code: str, output_dir: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Process a discovered logo using the logo_processor.py script.\n",
        "    \n",
        "    Returns:\n",
        "        Dict with processed logo paths and metadata\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        'success': False,\n",
        "        'stock_code': stock_code,\n",
        "        'output_files': [],\n",
        "        'has_icon': False,\n",
        "        'main_logo_path': None,\n",
        "        'icon_logo_path': None,\n",
        "        'svg_logo_path': None,\n",
        "        'error': None\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Handle SVG separately\n",
        "        if logo.is_svg or logo.format == 'svg':\n",
        "            return _process_svg_logo(logo, stock_code, output_dir)\n",
        "        \n",
        "        # Save original image to temp file\n",
        "        with tempfile.NamedTemporaryFile(suffix=f'.{logo.format}', delete=False) as tmp:\n",
        "            tmp.write(logo.image_data)\n",
        "            input_path = tmp.name\n",
        "        \n",
        "        try:\n",
        "            # Check if logo_processor.py exists\n",
        "            if not LOGO_PROCESSOR_PATH.exists():\n",
        "                # Fallback: just save the original image\n",
        "                main_path = os.path.join(output_dir, f'{stock_code}.png')\n",
        "                from PIL import Image\n",
        "                img = Image.open(BytesIO(logo.image_data)).convert('RGBA')\n",
        "                img.save(main_path, 'PNG')\n",
        "                result['success'] = True\n",
        "                result['output_files'] = [main_path]\n",
        "                result['main_logo_path'] = main_path\n",
        "                print(f\"  ‚ö† Logo processor not found, saved raw image\")\n",
        "                return result\n",
        "            \n",
        "            # Find venv Python if available\n",
        "            venv_python = ENRICHMENT_PROCESSOR_DIR / 'venv' / 'bin' / 'python3'\n",
        "            python_cmd = str(venv_python) if venv_python.exists() else 'python3'\n",
        "            \n",
        "            # Run logo_processor.py\n",
        "            cmd = [\n",
        "                python_cmd,\n",
        "                str(LOGO_PROCESSOR_PATH),\n",
        "                '--input', input_path,\n",
        "                '--output-dir', output_dir,\n",
        "                '--stock-code', stock_code\n",
        "            ]\n",
        "            \n",
        "            proc = subprocess.run(\n",
        "                cmd,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                cwd=str(ENRICHMENT_PROCESSOR_DIR),\n",
        "                timeout=120\n",
        "            )\n",
        "            \n",
        "            # Parse JSON output\n",
        "            if proc.stdout:\n",
        "                # Find JSON in output (might be mixed with stderr)\n",
        "                for line in proc.stdout.strip().split('\\n'):\n",
        "                    if line.strip().startswith('{'):\n",
        "                        try:\n",
        "                            parsed = json.loads(line)\n",
        "                            if parsed.get('success'):\n",
        "                                result['success'] = True\n",
        "                                result['output_files'] = parsed.get('output_files', [])\n",
        "                                result['has_icon'] = parsed.get('has_icon', False)\n",
        "                                \n",
        "                                # Find main and icon paths\n",
        "                                for path in result['output_files']:\n",
        "                                    if path.endswith(f'{stock_code}.png'):\n",
        "                                        result['main_logo_path'] = path\n",
        "                                    elif path.endswith(f'{stock_code}_icon.png'):\n",
        "                                        result['icon_logo_path'] = path\n",
        "                            else:\n",
        "                                result['error'] = parsed.get('error', 'Processing failed')\n",
        "                            break\n",
        "                        except json.JSONDecodeError:\n",
        "                            continue\n",
        "            \n",
        "            if not result['success'] and not result['error']:\n",
        "                result['error'] = proc.stderr or 'Unknown error'\n",
        "        \n",
        "        finally:\n",
        "            # Clean up temp file\n",
        "            try:\n",
        "                os.unlink(input_path)\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    except Exception as e:\n",
        "        result['error'] = str(e)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def _process_svg_logo(logo: DiscoveredLogo, stock_code: str, output_dir: str) -> Dict[str, Any]:\n",
        "    \"\"\"Process an SVG logo.\"\"\"\n",
        "    result = {\n",
        "        'success': False,\n",
        "        'stock_code': stock_code,\n",
        "        'output_files': [],\n",
        "        'has_icon': False,\n",
        "        'main_logo_path': None,\n",
        "        'icon_logo_path': None,\n",
        "        'svg_logo_path': None,\n",
        "        'error': None\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        svg_data = logo.svg_data if logo.svg_data else logo.image_data\n",
        "        \n",
        "        # Save SVG file\n",
        "        svg_path = os.path.join(output_dir, f'{stock_code}.svg')\n",
        "        with open(svg_path, 'wb') as f:\n",
        "            f.write(svg_data)\n",
        "        result['output_files'].append(svg_path)\n",
        "        result['svg_logo_path'] = svg_path\n",
        "        \n",
        "        # Try SVG text remover if available\n",
        "        if SVG_TEXT_REMOVER_PATH.exists():\n",
        "            venv_python = ENRICHMENT_PROCESSOR_DIR / 'venv' / 'bin' / 'python3'\n",
        "            python_cmd = str(venv_python) if venv_python.exists() else 'python3'\n",
        "            \n",
        "            cmd = [\n",
        "                python_cmd,\n",
        "                str(SVG_TEXT_REMOVER_PATH),\n",
        "                '--input', svg_path,\n",
        "                '--output-dir', output_dir,\n",
        "                '--stock-code', stock_code\n",
        "            ]\n",
        "            \n",
        "            proc = subprocess.run(\n",
        "                cmd,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                cwd=str(ENRICHMENT_PROCESSOR_DIR),\n",
        "                timeout=60\n",
        "            )\n",
        "            \n",
        "            # Parse output\n",
        "            if proc.stdout:\n",
        "                for line in proc.stdout.strip().split('\\n'):\n",
        "                    if line.strip().startswith('{'):\n",
        "                        try:\n",
        "                            parsed = json.loads(line)\n",
        "                            if parsed.get('success'):\n",
        "                                result['output_files'].extend(parsed.get('output_files', []))\n",
        "                                if parsed.get('icon_svg_path'):\n",
        "                                    result['has_icon'] = True\n",
        "                                    result['icon_logo_path'] = parsed.get('icon_png_path')\n",
        "                            break\n",
        "                        except json.JSONDecodeError:\n",
        "                            continue\n",
        "        \n",
        "        # Render SVG to PNG using cairosvg\n",
        "        try:\n",
        "            import cairosvg\n",
        "            png_path = os.path.join(output_dir, f'{stock_code}.png')\n",
        "            cairosvg.svg2png(url=svg_path, write_to=png_path, output_width=256)\n",
        "            result['output_files'].append(png_path)\n",
        "            result['main_logo_path'] = png_path\n",
        "        except ImportError:\n",
        "            print(f\"  ‚ö† cairosvg not available, skipping PNG render\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö† SVG to PNG render failed: {e}\")\n",
        "        \n",
        "        result['success'] = len(result['output_files']) > 0\n",
        "    \n",
        "    except Exception as e:\n",
        "        result['error'] = str(e)\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"‚úì Logo processing functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: GCS Upload Functions\n",
        "\n",
        "def upload_logos_to_gcs(processed_result: Dict[str, Any], stock_code: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Upload processed logos to Google Cloud Storage.\n",
        "    \n",
        "    Returns:\n",
        "        Dict with GCS URLs for main_logo, icon_logo, svg_logo\n",
        "    \"\"\"\n",
        "    urls = {\n",
        "        'main_logo_url': '',\n",
        "        'icon_logo_url': '',\n",
        "        'svg_logo_url': ''\n",
        "    }\n",
        "    \n",
        "    if not processed_result.get('success'):\n",
        "        return urls\n",
        "    \n",
        "    try:\n",
        "        from google.cloud import storage\n",
        "        \n",
        "        client = storage.Client()\n",
        "        bucket = client.bucket(GCS_BUCKET)\n",
        "        \n",
        "        for file_path in processed_result.get('output_files', []):\n",
        "            if not os.path.exists(file_path):\n",
        "                continue\n",
        "            \n",
        "            filename = os.path.basename(file_path)\n",
        "            \n",
        "            # Determine content type and object path\n",
        "            if filename.endswith('.svg'):\n",
        "                content_type = 'image/svg+xml'\n",
        "                object_name = f'logos/svg/{filename}'\n",
        "            else:\n",
        "                content_type = 'image/png'\n",
        "                object_name = f'logos/{filename}'\n",
        "            \n",
        "            # Upload to GCS\n",
        "            blob = bucket.blob(object_name)\n",
        "            blob.upload_from_filename(file_path, content_type=content_type)\n",
        "            blob.cache_control = 'public, max-age=31536000'  # 1 year cache\n",
        "            blob.patch()\n",
        "            \n",
        "            gcs_url = f'https://storage.googleapis.com/{GCS_BUCKET}/{object_name}'\n",
        "            \n",
        "            # Track URL types\n",
        "            if filename == f'{stock_code}.png':\n",
        "                urls['main_logo_url'] = gcs_url\n",
        "            elif filename == f'{stock_code}_icon.png':\n",
        "                urls['icon_logo_url'] = gcs_url\n",
        "            elif filename == f'{stock_code}.svg' or filename == f'{stock_code}_icon.svg':\n",
        "                if not urls['svg_logo_url'] or '_icon' in filename:\n",
        "                    urls['svg_logo_url'] = gcs_url\n",
        "            \n",
        "            print(f\"  ‚òÅ Uploaded: {object_name}\")\n",
        "    \n",
        "    except ImportError:\n",
        "        print(f\"  ‚ö† google-cloud-storage not installed, skipping GCS upload\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö† GCS upload failed: {e}\")\n",
        "    \n",
        "    return urls\n",
        "\n",
        "print(\"‚úì GCS upload functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Company Metadata Enrichment\n",
        "\n",
        "GPT-based enrichment for company profiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: GPT Enrichment Schema\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a financial analyst specializing in Australian Stock Exchange (ASX) companies.\n",
        "\n",
        "Return ONLY valid JSON matching the requested schema. No markdown. No commentary.\n",
        "\n",
        "Quality rules:\n",
        "- Be specific and factual; avoid generic template language.\n",
        "- If a field is truly unavailable, use null (for strings/objects) or [] (for arrays).\n",
        "- Provide exactly 5 tags.\n",
        "\"\"\"\n",
        "\n",
        "def enrich_company_metadata(company: pd.Series) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Enrich company metadata using GPT.\n",
        "    \"\"\"\n",
        "    stock_code = company['stock_code']\n",
        "    company_name = company['company_name']\n",
        "    \n",
        "    user_prompt = f\"\"\"\n",
        "<company_context>\n",
        "Company Name: {company_name}\n",
        "Stock Code: {stock_code}\n",
        "Industry: {company.get('industry', 'N/A')}\n",
        "Website: {company.get('website', 'N/A')}\n",
        "Current Summary: {company.get('summary', 'N/A')}\n",
        "</company_context>\n",
        "\n",
        "Return a JSON object with this EXACT structure (valid JSON only, no markdown):\n",
        "\n",
        "{{\n",
        "  \"tags\": [\"tag1\", \"tag2\", \"tag3\", \"tag4\", \"tag5\"],\n",
        "  \"enhanced_summary\": \"2-4 sentences covering business model, market position, unique value\",\n",
        "  \"company_history\": \"3-5 sentences on founding, evolution, major milestones\",\n",
        "  \"key_people\": [\n",
        "    {{\"name\": \"Full Name\", \"role\": \"CEO\", \"bio\": \"1-2 sentence bio\"}},\n",
        "    {{\"name\": \"Full Name\", \"role\": \"CFO\", \"bio\": \"1-2 sentence bio\"}}\n",
        "  ],\n",
        "  \"competitive_advantages\": \"2-3 specific competitive advantages with detail\",\n",
        "  \"risk_factors\": [\"Specific risk 1\", \"Specific risk 2\", \"Specific risk 3\"],\n",
        "  \"recent_developments\": \"Recent developments from the last ~6 months\",\n",
        "  \"social_media_links\": {{\n",
        "    \"linkedin\": \"https://linkedin.com/company/...\",\n",
        "    \"twitter\": \"https://twitter.com/...\"\n",
        "  }}\n",
        "}}\n",
        "\"\"\"\n",
        "    \n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "                temperature=0.2,\n",
        "                max_tokens=2000\n",
        "            )\n",
        "            \n",
        "            raw = response.choices[0].message.content.strip()\n",
        "            enriched_data = json.loads(raw)\n",
        "            enriched_data['stock_code'] = stock_code\n",
        "            enriched_data['enrichment_status'] = 'completed'\n",
        "            enriched_data['enrichment_date'] = datetime.now().isoformat()\n",
        "            \n",
        "            return enriched_data\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö† Error enriching {stock_code} (attempt {attempt + 1}/{MAX_RETRIES}): {e}\")\n",
        "            if attempt < MAX_RETRIES - 1:\n",
        "                time.sleep(RETRY_DELAY)\n",
        "    \n",
        "    return {\n",
        "        'stock_code': stock_code,\n",
        "        'enrichment_status': 'failed',\n",
        "        'enrichment_error': 'Failed after maximum retries'\n",
        "    }\n",
        "\n",
        "print(\"‚úì GPT enrichment function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Data Fetching\n",
        "\n",
        "def fetch_stocks_to_enrich() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch stocks that need enrichment from the database.\n",
        "    Prioritizes stocks that haven't been enriched or need logo updates.\n",
        "    \"\"\"\n",
        "    engine = create_engine(DATABASE_URL)\n",
        "    \n",
        "    # Fetch stocks from company-metadata table\n",
        "    query = \"\"\"\n",
        "    SELECT \n",
        "        stock_code,\n",
        "        company_name,\n",
        "        industry,\n",
        "        website,\n",
        "        summary,\n",
        "        enrichment_status,\n",
        "        logo_gcs_url\n",
        "    FROM \"company-metadata\"\n",
        "    WHERE stock_code IS NOT NULL\n",
        "    ORDER BY \n",
        "        CASE \n",
        "            WHEN enrichment_status IS NULL OR enrichment_status = '' THEN 0\n",
        "            WHEN enrichment_status = 'pending' THEN 1\n",
        "            WHEN enrichment_status = 'failed' THEN 2\n",
        "            ELSE 3\n",
        "        END,\n",
        "        company_name\n",
        "    \"\"\"\n",
        "    \n",
        "    df = pd.read_sql(query, engine)\n",
        "    engine.dispose()\n",
        "    \n",
        "    # Count status distribution\n",
        "    status_counts = df['enrichment_status'].value_counts(dropna=False)\n",
        "    needs_enrichment = df[df['enrichment_status'].isna() | (df['enrichment_status'] == '') | (df['enrichment_status'] == 'pending')]\n",
        "    needs_logo = df[df['logo_gcs_url'].isna() | (df['logo_gcs_url'] == '')]\n",
        "    \n",
        "    print(f\"‚úì Fetched {len(df)} stocks from database\")\n",
        "    print(f\"  - Needs enrichment: {len(needs_enrichment)}\")\n",
        "    print(f\"  - Needs logo: {len(needs_logo)}\")\n",
        "    print(f\"\\nStatus distribution:\")\n",
        "    print(status_counts)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def load_checkpoint() -> Dict[str, Any]:\n",
        "    \"\"\"Load checkpoint data to resume processing.\"\"\"\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        with open(CHECKPOINT_FILE, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "        print(f\"‚úì Loaded checkpoint: {checkpoint.get('processed_count', 0)} companies processed\")\n",
        "        return checkpoint\n",
        "    return {'processed_count': 0, 'processed_codes': [], 'logo_processed_codes': []}\n",
        "\n",
        "def save_checkpoint(checkpoint: Dict[str, Any]):\n",
        "    \"\"\"Save checkpoint data for resumption.\"\"\"\n",
        "    os.makedirs(os.path.dirname(CHECKPOINT_FILE), exist_ok=True)\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump(checkpoint, f, indent=2)\n",
        "\n",
        "# Fetch data\n",
        "df_stocks = fetch_stocks_to_enrich()\n",
        "checkpoint = load_checkpoint()\n",
        "\n",
        "print(f\"\\nDataFrame shape: {df_stocks.shape}\")\n",
        "df_stocks.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Main Enrichment Pipeline\n",
        "\n",
        "def enrich_company_full(company: pd.Series, logo_output_dir: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Perform full enrichment for a company:\n",
        "    1. GPT metadata enrichment\n",
        "    2. Logo discovery\n",
        "    3. Logo processing\n",
        "    4. GCS upload\n",
        "    \"\"\"\n",
        "    stock_code = company['stock_code']\n",
        "    company_name = company['company_name']\n",
        "    website = company.get('website', '')\n",
        "    \n",
        "    result = {\n",
        "        'stock_code': stock_code,\n",
        "        'company_name': company_name,\n",
        "        'enrichment_status': 'pending',\n",
        "        'logo_status': 'pending',\n",
        "        'logo_gcs_url': '',\n",
        "        'logo_icon_gcs_url': '',\n",
        "        'logo_svg_gcs_url': '',\n",
        "    }\n",
        "    \n",
        "    # 1. GPT Metadata Enrichment\n",
        "    print(f\"  üìù Enriching metadata...\")\n",
        "    metadata = enrich_company_metadata(company)\n",
        "    result.update(metadata)\n",
        "    \n",
        "    # 2. Logo Discovery\n",
        "    if website and website.strip():\n",
        "        print(f\"  üîç Discovering logo...\")\n",
        "        logo_discoverer = LogoDiscoverer()\n",
        "        logo = logo_discoverer.discover_logo(website, company_name, stock_code)\n",
        "        \n",
        "        if logo:\n",
        "            # 3. Logo Processing\n",
        "            print(f\"  üé® Processing logo...\")\n",
        "            stock_logo_dir = os.path.join(logo_output_dir, stock_code)\n",
        "            processed = process_logo(logo, stock_code, stock_logo_dir)\n",
        "            \n",
        "            if processed.get('success'):\n",
        "                # 4. GCS Upload\n",
        "                print(f\"  ‚òÅ Uploading to GCS...\")\n",
        "                gcs_urls = upload_logos_to_gcs(processed, stock_code)\n",
        "                \n",
        "                result['logo_status'] = 'completed'\n",
        "                result['logo_gcs_url'] = gcs_urls.get('main_logo_url', '')\n",
        "                result['logo_icon_gcs_url'] = gcs_urls.get('icon_logo_url', '')\n",
        "                result['logo_svg_gcs_url'] = gcs_urls.get('svg_logo_url', '')\n",
        "                result['logo_source_url'] = logo.source_url\n",
        "                result['logo_format'] = logo.format\n",
        "            else:\n",
        "                result['logo_status'] = 'failed'\n",
        "                result['logo_error'] = processed.get('error', 'Processing failed')\n",
        "        else:\n",
        "            result['logo_status'] = 'not_found'\n",
        "    else:\n",
        "        result['logo_status'] = 'no_website'\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"‚úì Full enrichment pipeline defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Database Update\n",
        "\n",
        "def update_database(results: List[Dict[str, Any]]):\n",
        "    \"\"\"\n",
        "    Update the database with enrichment results.\n",
        "    \"\"\"\n",
        "    engine = create_engine(DATABASE_URL)\n",
        "    updated_count = 0\n",
        "    failed_count = 0\n",
        "    \n",
        "    print(f\"\\nüíæ Updating database with {len(results)} records...\")\n",
        "    \n",
        "    with engine.connect() as conn:\n",
        "        for result in tqdm(results):\n",
        "            stock_code = result.get('stock_code')\n",
        "            if not stock_code:\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                update_data = {\n",
        "                    'tags': result.get('tags', []) or None,\n",
        "                    'enhanced_summary': result.get('enhanced_summary'),\n",
        "                    'company_history': result.get('company_history'),\n",
        "                    'key_people': json.dumps(result.get('key_people', [])),\n",
        "                    'competitive_advantages': result.get('competitive_advantages'),\n",
        "                    'risk_factors': json.dumps(result.get('risk_factors', [])) if isinstance(result.get('risk_factors'), list) else result.get('risk_factors'),\n",
        "                    'recent_developments': result.get('recent_developments'),\n",
        "                    'social_media_links': json.dumps(result.get('social_media_links', {})),\n",
        "                    'logo_gcs_url': result.get('logo_gcs_url'),\n",
        "                    'logo_icon_gcs_url': result.get('logo_icon_gcs_url'),\n",
        "                    'logo_svg_gcs_url': result.get('logo_svg_gcs_url'),\n",
        "                    'logo_source_url': result.get('logo_source_url'),\n",
        "                    'enrichment_status': result.get('enrichment_status', 'completed'),\n",
        "                    'enrichment_date': result.get('enrichment_date', datetime.now().isoformat()),\n",
        "                    'enrichment_error': result.get('enrichment_error'),\n",
        "                    'stock_code': stock_code\n",
        "                }\n",
        "                \n",
        "                query = text(\"\"\"\n",
        "                    UPDATE \"company-metadata\"\n",
        "                    SET \n",
        "                        tags = :tags,\n",
        "                        enhanced_summary = :enhanced_summary,\n",
        "                        company_history = :company_history,\n",
        "                        key_people = :key_people,\n",
        "                        competitive_advantages = :competitive_advantages,\n",
        "                        risk_factors = :risk_factors,\n",
        "                        recent_developments = :recent_developments,\n",
        "                        social_media_links = :social_media_links,\n",
        "                        logo_gcs_url = :logo_gcs_url,\n",
        "                        logo_icon_gcs_url = :logo_icon_gcs_url,\n",
        "                        logo_svg_gcs_url = :logo_svg_gcs_url,\n",
        "                        logo_source_url = :logo_source_url,\n",
        "                        enrichment_status = :enrichment_status,\n",
        "                        enrichment_date = :enrichment_date,\n",
        "                        enrichment_error = :enrichment_error\n",
        "                    WHERE stock_code = :stock_code\n",
        "                \"\"\")\n",
        "                \n",
        "                conn.execute(query, update_data)\n",
        "                conn.commit()\n",
        "                updated_count += 1\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ùå Failed to update {stock_code}: {e}\")\n",
        "                failed_count += 1\n",
        "    \n",
        "    engine.dispose()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Database update complete:\")\n",
        "    print(f\"  - Updated: {updated_count}\")\n",
        "    print(f\"  - Failed: {failed_count}\")\n",
        "\n",
        "print(\"‚úì Database update function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Batch Processing\n",
        "\n",
        "def process_batch(df: pd.DataFrame, checkpoint: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Process a batch of companies with checkpoint support.\n",
        "    \"\"\"\n",
        "    # Filter companies to process\n",
        "    companies_to_process = df[~df['stock_code'].isin(checkpoint.get('processed_codes', []))]\n",
        "    \n",
        "    if PROCESS_SUBSET:\n",
        "        companies_to_process = companies_to_process.head(SUBSET_SIZE)\n",
        "        print(f\"\\nüìã Processing subset of {len(companies_to_process)} companies\")\n",
        "    else:\n",
        "        print(f\"\\nüìã Processing {len(companies_to_process)} companies (full dataset)\")\n",
        "    \n",
        "    # Create output directory for logos\n",
        "    logo_output_dir = 'data/logos_processed'\n",
        "    os.makedirs(logo_output_dir, exist_ok=True)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for idx, (_, company) in enumerate(tqdm(companies_to_process.iterrows(), total=len(companies_to_process))):\n",
        "        stock_code = company['stock_code']\n",
        "        \n",
        "        try:\n",
        "            print(f\"\\nüîç [{idx+1}/{len(companies_to_process)}] Processing {stock_code} - {company['company_name']}\")\n",
        "            \n",
        "            # Full enrichment\n",
        "            result = enrich_company_full(company, logo_output_dir)\n",
        "            results.append(result)\n",
        "            \n",
        "            # Update checkpoint\n",
        "            checkpoint['processed_codes'].append(stock_code)\n",
        "            checkpoint['processed_count'] = len(checkpoint['processed_codes'])\n",
        "            \n",
        "            # Save checkpoint periodically\n",
        "            if (idx + 1) % CHECKPOINT_INTERVAL == 0:\n",
        "                save_checkpoint(checkpoint)\n",
        "                print(f\"\\nüíæ Checkpoint saved: {checkpoint['processed_count']} companies\")\n",
        "            \n",
        "            # Rate limiting\n",
        "            time.sleep(1)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to process {stock_code}: {e}\")\n",
        "            results.append({\n",
        "                'stock_code': stock_code,\n",
        "                'enrichment_status': 'failed',\n",
        "                'enrichment_error': str(e)\n",
        "            })\n",
        "    \n",
        "    # Final checkpoint save\n",
        "    save_checkpoint(checkpoint)\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"‚úì Batch processing function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: Run the Pipeline\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ STARTING BATCH ENRICHMENT PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Process companies\n",
        "results = process_batch(df_stocks, checkpoint)\n",
        "\n",
        "# Update database\n",
        "update_database(results)\n",
        "\n",
        "# Generate summary\n",
        "total = len(results)\n",
        "enrichment_completed = sum(1 for r in results if r.get('enrichment_status') == 'completed')\n",
        "logo_completed = sum(1 for r in results if r.get('logo_status') == 'completed')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ PIPELINE COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nResults Summary:\")\n",
        "print(f\"  - Total processed: {total}\")\n",
        "print(f\"  - Metadata enriched: {enrichment_completed} ({enrichment_completed/total*100:.1f}%)\")\n",
        "print(f\"  - Logos processed: {logo_completed} ({logo_completed/total*100:.1f}%)\")\n",
        "\n",
        "# Export results\n",
        "df_results = pd.DataFrame(results)\n",
        "os.makedirs(os.path.dirname(RESULTS_FILE), exist_ok=True)\n",
        "df_results.to_csv(RESULTS_FILE, index=False)\n",
        "print(f\"\\nüíæ Results exported to: {RESULTS_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: Results Analysis\n",
        "\n",
        "if 'df_results' in locals():\n",
        "    print(\"\\nüìä ENRICHMENT RESULTS ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Enrichment status\n",
        "    print(\"\\nEnrichment Status:\")\n",
        "    print(df_results['enrichment_status'].value_counts())\n",
        "    \n",
        "    # Logo status\n",
        "    print(\"\\nLogo Status:\")\n",
        "    print(df_results['logo_status'].value_counts())\n",
        "    \n",
        "    # Logo formats\n",
        "    if 'logo_format' in df_results.columns:\n",
        "        print(\"\\nLogo Formats:\")\n",
        "        print(df_results['logo_format'].value_counts(dropna=False))\n",
        "    \n",
        "    # Tags distribution\n",
        "    if 'tags' in df_results.columns:\n",
        "        all_tags = []\n",
        "        for tags in df_results['tags'].dropna():\n",
        "            if isinstance(tags, list):\n",
        "                all_tags.extend(tags)\n",
        "        \n",
        "        from collections import Counter\n",
        "        tag_freq = Counter(all_tags)\n",
        "        print(f\"\\nUnique Tags: {len(tag_freq)}\")\n",
        "        print(\"\\nMost Common Tags:\")\n",
        "        for tag, count in tag_freq.most_common(15):\n",
        "            print(f\"  {tag}: {count}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Individual Components\n",
        "\n",
        "Test the enrichment pipeline on individual companies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 14: Test Logo Discovery\n",
        "\n",
        "# Test logo discovery on a specific company\n",
        "TEST_STOCK_CODE = 'CBA'  # Commonwealth Bank of Australia\n",
        "\n",
        "test_company = df_stocks[df_stocks['stock_code'] == TEST_STOCK_CODE]\n",
        "if len(test_company) > 0:\n",
        "    company = test_company.iloc[0]\n",
        "    website = company.get('website', '')\n",
        "    \n",
        "    print(f\"Testing logo discovery for: {TEST_STOCK_CODE}\")\n",
        "    print(f\"Website: {website}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    scraper = LogoScraper(company_name=company['company_name'])\n",
        "    candidates = scraper.scrape_logos(website)\n",
        "    \n",
        "    print(f\"\\nFound {len(candidates)} logo candidates:\")\n",
        "    for i, c in enumerate(candidates[:10]):\n",
        "        print(f\"\\n{i+1}. Score: {c.score:.1f}\")\n",
        "        print(f\"   URL: {c.url[:80]}...\")\n",
        "        print(f\"   Format: {c.format}, Source: {c.source}\")\n",
        "        if c.alt:\n",
        "            print(f\"   Alt: {c.alt[:50]}\")\n",
        "else:\n",
        "    print(f\"Stock code {TEST_STOCK_CODE} not found in database\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 15: Test Full Enrichment on Single Company\n",
        "\n",
        "TEST_STOCK_CODE = 'BHP'  # Change this to test different companies\n",
        "\n",
        "test_company = df_stocks[df_stocks['stock_code'] == TEST_STOCK_CODE]\n",
        "if len(test_company) > 0:\n",
        "    company = test_company.iloc[0]\n",
        "    \n",
        "    print(f\"\\nTesting full enrichment for: {TEST_STOCK_CODE} - {company['company_name']}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Run full enrichment\n",
        "    result = enrich_company_full(company, 'data/test_logos')\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ENRICHMENT RESULT:\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(f\"\\nEnrichment Status: {result.get('enrichment_status')}\")\n",
        "    print(f\"Logo Status: {result.get('logo_status')}\")\n",
        "    \n",
        "    if result.get('tags'):\n",
        "        print(f\"\\nTags: {result.get('tags')}\")\n",
        "    \n",
        "    if result.get('enhanced_summary'):\n",
        "        print(f\"\\nEnhanced Summary:\\n{result.get('enhanced_summary')[:500]}...\")\n",
        "    \n",
        "    if result.get('logo_gcs_url'):\n",
        "        print(f\"\\nLogo URLs:\")\n",
        "        print(f\"  Main: {result.get('logo_gcs_url')}\")\n",
        "        print(f\"  Icon: {result.get('logo_icon_gcs_url')}\")\n",
        "        print(f\"  SVG:  {result.get('logo_svg_gcs_url')}\")\n",
        "else:\n",
        "    print(f\"Stock code {TEST_STOCK_CODE} not found in database\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
