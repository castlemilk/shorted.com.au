{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7728c150-6b6a-4722-bbbd-d275fbe42a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /Users/benebsworth/projects/shorted/venv/lib/python3.11/site-packages (0.27.0)\n",
      "Requirement already satisfied: tqdm in /Users/benebsworth/projects/shorted/venv/lib/python3.11/site-packages (4.66.2)\n",
      "Requirement already satisfied: anyio in /Users/benebsworth/projects/shorted/venv/lib/python3.11/site-packages (from httpx) (4.3.0)\n",
      "Requirement already satisfied: certifi in /Users/benebsworth/projects/shorted/venv/lib/python3.11/site-packages (from httpx) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/benebsworth/projects/shorted/venv/lib/python3.11/site-packages (from httpx) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/benebsworth/projects/shorted/venv/lib/python3.11/site-packages (from httpx) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/benebsworth/projects/shorted/venv/lib/python3.11/site-packages (from httpx) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/benebsworth/projects/shorted/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install httpx tqdm dask-expr dask chardet pandas matplotlib sqlalchemy psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8966e43d-c0a3-4923-84c3-93f853a81374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import dask\n",
    "from pathlib import Path\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import chardet\n",
    "\n",
    "# Define the directory to store CSV files and ensure it exists\n",
    "SHORTS_DATA_DIRECTORY = \"data/shorts\"\n",
    "if not os.path.exists(SHORTS_DATA_DIRECTORY):\n",
    "    os.makedirs(SHORTS_DATA_DIRECTORY)\n",
    "\n",
    "# URLs for fetching the JSON data and the base URL for downloads\n",
    "data_url = \"https://download.asic.gov.au/short-selling/short-selling-data.json\"\n",
    "base_url = \"https://download.asic.gov.au/short-selling/\"\n",
    "\n",
    "# Initialize an HTTP client\n",
    "client = httpx.Client()\n",
    "\n",
    "# Fetch the list of downloadable CSVs\n",
    "response = client.get(data_url)\n",
    "short_selling_data = response.json()\n",
    "\n",
    "\n",
    "\n",
    "def generate_download_url(record):\n",
    "    \"\"\"Generate download URL for each record.\"\"\"\n",
    "    date_str = str(record['date'])\n",
    "    year, month, day = date_str[:4], date_str[4:6], date_str[6:]\n",
    "    return f\"{base_url}RR{year}{month}{day}-{record['version']}-SSDailyAggShortPos.csv\"\n",
    "\n",
    "def download_file(url, file_path, progress_bar):\n",
    "    \"\"\"Download a file from a given URL to a specified path.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        with client.stream(\"GET\", url) as response:\n",
    "            response.raise_for_status()\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_bytes(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        progress_bar.update(1)\n",
    "    else:\n",
    "        progress_bar.update(1)\n",
    "\n",
    "def download_records():\n",
    "    \"\"\"\n",
    "    Download the short selling data from the ASIC website.\n",
    "    \"\"\"\n",
    "    # Initialize progress bar\n",
    "    progress_bar = tqdm(total=len(short_selling_data))\n",
    "    # Iterate through the records and download the CSV files\n",
    "    for record in short_selling_data:\n",
    "        file_url = generate_download_url(record)\n",
    "        file_name = file_url.split('/')[-1]\n",
    "        file_path = os.path.join(SHORTS_DATA_DIRECTORY, file_name)\n",
    "        \n",
    "        # Download the file if it does not already exist\n",
    "        download_file(file_url, file_path, progress_bar=progress_bar)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "def read_csv_smart(file_path, expected_schema: dict):\n",
    "    \"\"\"\n",
    "    Read an individual short data report for a given day in CSV format and normalises to the defined schema.\n",
    "    \"\"\"\n",
    "    expected_columns = list(expected_schema.keys())\n",
    "    # Detect file encoding\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read(10000))\n",
    "    encoding = result['encoding']\n",
    "\n",
    "    try:\n",
    "        date_str = ''.join(filter(str.isdigit, file_path.name.split('-')[0]))\n",
    "        df = pd.read_csv(file_path, encoding=encoding, engine='python', sep=None)\n",
    "        # Normalize column names\n",
    "        df.columns = df.columns.str.upper().str.strip().str.replace(' ', '_').str.replace('%', 'PERCENT')\n",
    "            \n",
    "        # Ensure all expected columns are present, even if they're missing in the CSV, and reorder to match expected schema\n",
    "        for column in expected_columns:\n",
    "            if column not in df.columns:\n",
    "                df[column] = pd.NA\n",
    "        df = df[expected_columns]\n",
    "\n",
    "        # Convert columns to expected types\n",
    "        for column, dtype in expected_schema.items():\n",
    "            df[column] = df[column].astype(dtype, errors='ignore')\n",
    "          # Convert '%_OF_TOTAL_PRODUCT_IN_ISSUE_REPORTED_AS_SHORT_POSITIONS' to float64, coercing errors\n",
    "        if 'PERCENT_OF_TOTAL_PRODUCT_IN_ISSUE_REPORTED_AS_SHORT_POSITIONS' in df.columns:\n",
    "            df['PERCENT_OF_TOTAL_PRODUCT_IN_ISSUE_REPORTED_AS_SHORT_POSITIONS'] = pd.to_numeric(df['PERCENT_OF_TOTAL_PRODUCT_IN_ISSUE_REPORTED_AS_SHORT_POSITIONS'], errors='coerce')\n",
    "        df['DATE'] = pd.to_datetime(date_str, format='%Y%m%d')\n",
    "\n",
    "        # strip white space\n",
    "        df['PRODUCT_CODE']=df['PRODUCT_CODE'].str.strip()\n",
    "        df['PRODUCT']=df['PRODUCT'].str.strip()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {file_path} with encoding {encoding}: {e}\")\n",
    "        # Return an empty DataFrame with expected columns if reading fails\n",
    "        return pd.DataFrame(columns=expected_columns).astype(expected_schema)\n",
    "    \n",
    "def process_short_data_into_dataframe():\n",
    "    \"\"\"\n",
    "    Read all the downloaded short selling data into a DataFrame.\n",
    "    \"\"\"\n",
    "    # Define the expected schema (column names and their order)\n",
    "    expected_schema = {\n",
    "        'DATE': 'datetime64[ns]',\n",
    "        'PRODUCT': 'object',\n",
    "        'PRODUCT_CODE': 'object',\n",
    "        'REPORTED_SHORT_POSITIONS': 'float64',\n",
    "        'TOTAL_PRODUCT_IN_ISSUE': 'float64',\n",
    "        'PERCENT_OF_TOTAL_PRODUCT_IN_ISSUE_REPORTED_AS_SHORT_POSITIONS': 'float64'\n",
    "    }\n",
    "    expected_columns = list(expected_schema.keys())\n",
    "    # Collect CSV files and read them using Dask\n",
    "    csv_files = list(Path(SHORTS_DATA_DIRECTORY).glob('*.csv'))\n",
    "    delayed_readings = [dask.delayed(read_csv_smart)(file, expected_schema) for file in csv_files]\n",
    "    # Use the defined schema to create the meta DataFrame for Dask\n",
    "    meta_df = pd.DataFrame(columns=expected_columns).astype(expected_schema)\n",
    "\n",
    "    # Create a Dask DataFrame from the delayed objects\n",
    "    ddf = dd.from_delayed(delayed_readings, meta=meta_df)\n",
    "\n",
    "    # Compute the Dask DataFrame to get the final pandas DataFrame\n",
    "    with ProgressBar():\n",
    "        agg_df = ddf.compute()\n",
    "    return agg_df\n",
    "\n",
    "def write_short_data_to_postgres(df, table_name, connection_string):\n",
    "    \"\"\"\n",
    "    Write the short selling data to a PostgreSQL database.\n",
    "    \"\"\"\n",
    "    from sqlalchemy import create_engine\n",
    "    engine = create_engine(connection_string)\n",
    "    df.to_sql(table_name, engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fd5bfc2-e768-444a-87a0-864cb3bdfb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3495/3495 [00:00<00:00, 238891.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 12.51 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download daily short selling data\n",
    "download_records()\n",
    "\n",
    "# Process all CSV files into a DataFrame\n",
    "short_selling_df = process_short_data_into_dataframe()\n",
    "\n",
    "# write data to postgres\n",
    "write_short_data_to_postgres(short_selling_df, 'shorts', os.environ.get('DATABASE_URL', \"postgresql://admin:password@localhost:5432/shorts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2bc82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
